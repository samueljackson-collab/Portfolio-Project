# AI Prompt Library — Red Team Portfolio Projects

Each portfolio project below is part of a 25-project cybersecurity portfolio spanning Red Team, Blue Team, Cloud Security, DevOps, and GRC roles. For each project, we first identify missing, broken, or incomplete components (code, configuration, documentation, tests, etc.), then provide a detailed prompt for an AI code generator (OpenAI Codex or Anthropic Claude) to produce a full-stack, production-grade implementation. These prompts are structured to be self-contained and deterministic, following the Portfolio Project Foreman conventions (FastAPI backend with /health endpoint, React frontend when applicable, Terraform IaC, comprehensive tests, CI/CD via GitHub Actions, Docker setup, and thorough documentation). The prompts explicitly instruct the AI to include in-code comments and external documentation explaining what the code does and why, ensuring clarity of each code component’s purpose. Pinned versions and consistent naming are enforced for deterministic outputs. You can copy each prompt directly into Codex or Claude; the AI’s response should yield a complete, working project (file structure, code, tests, docs, CI) that fills all identified gaps.

## 1. External Infrastructure Penetration Test (Red Team)

**Gaps:** This project lacks any implemented code or runnable service – currently only high-level findings and a report outline exist. There is no backend API or database to catalog discovered vulnerabilities, no frontend to visualize results, and no automation to simulate scanning. Infrastructure-as-code and CI/CD pipelines are absent. Documentation exists only as an outline of the engagement, with no usage instructions. Testing is minimal or nonexistent (no unit tests for scanning logic or data models). In summary, we need to build the entire application and support infrastructure from scratch.

**AI Prompt:**

You are an expert Full Stack Developer and Security Engineer. Create a **Portfolio Project - External Infrastructure Penetration Test** implementation. 

**Scope:** Build a production-grade web application (FastAPI backend + React frontend) that allows users to **record and review penetration test results for external infrastructure**. Include:
- A FastAPI **backend** (Python) with a PostgreSQL database (SQLAlchemy + Alembic migrations). Design models for **Targets** (e.g., IP or domain), **Findings** (vulnerabilities discovered), and **Exploitation** attempts. Implement REST endpoints to create, read, update, and delete (CRUD) Targets and Findings. Include a `POST /scan` endpoint that simulates a network scan of a target (for demo purposes, generate a few dummy Findings for the given Target). Ensure the backend implements authentication (JWT-based) for any write operations, and a public read-only mode for demonstration. **Include a `/health` endpoint** returning HTTP 200 for health checks. Implement input validation and error handling (e.g., target IP format).
- A React **frontend** (TypeScript + Vite) that allows a pentester to input new Targets (with IP or domain) and trigger a “Scan”. Display the list of Targets and their Findings in a dashboard. For each Target, show key info (IP, status, etc.) and a table of associated Findings (with severity, description, status). Implement forms and client-side validation for adding Targets. Provide a login page for the pentester (just simulate login to get a JWT from the backend) and protect the input forms (read-only view if not logged in). Use a modern UI library or Tailwind CSS for styling. Ensure the frontend calls the backend API asynchronously (using Axios or Fetch) and handles loading states and errors gracefully.
- **Testing:** Write unit tests for critical backend logic (e.g., a utility function to calculate a severity score or generate dummy findings, the scan simulation, and the `/health` endpoint). Use PyTest for Python tests. Also include a couple of integration tests hitting the API (possibly using `httpx` or FastAPI’s test client). For the frontend, include a few basic Jest tests or React Testing Library tests (e.g., rendering the dashboard, form input validation).
- **CI/CD:** Provide a GitHub Actions workflow (`.github/workflows/ci.yml`) that sets up Python and Node environments, installs dependencies, runs backend tests (`pytest`) and frontend build/tests, lints the code (e.g., using flake8 and ESLint), and builds Docker images. Ensure pinned versions for actions and dependencies for deterministic builds.
- **Infrastructure as Code:** Include Terraform scripts to provision cloud infrastructure for this service in AWS. For example, Terraform code to create an ECS or EC2 instance for the FastAPI app, an S3 bucket or CloudFront for serving the React app, a PostgreSQL RDS database, and the necessary security group and IAM roles. Use variables for things like environment and enable easy deployment. Provide a `terraform.tfvars.example` file with placeholder values. (If any part of infra is unspecified, choose sane defaults and **scaffold minimal stub resources to complete the stack**).
- **Docker setup:** Write Dockerfiles for the backend (Python 3.11 slim, installing dependencies and running Uvicorn/gunicorn server) and for the frontend (Node 20, build the React app and serve it via nginx or a simple node server). Also include a `docker-compose.yml` to orchestrate the whole application (backend, frontend, and database) for local development/testing.
- **Documentation:** Provide a **README.md** at the project root (`projects/external-pen-test/README.md`) explaining the project’s purpose, how to set up and run it (including migrations and any initial data), and how to execute a sample scan. Include instructions for running via Docker Compose and via local environment. Also include a **Quickstart** section to get the app up quickly, and mention the `/health` endpoint for monitoring. Additionally, write a brief **Architecture & Design** section describing how the scanning simulation works and how data flows between frontend and backend.
- **Comments & Explanations:** Include **detailed comments in code** (especially for non-obvious logic like the dummy scan generation and security considerations in authentication) to explain what each part does and why. In the README, include an **“Implementation Details”** section summarizing key technical decisions (e.g., choice of FastAPI, JWT, how vulnerabilities are represented and scored). 

Use **consistent naming** and ensure no dangling references (the frontend endpoints should match backend routes, etc.). All produced code should follow best practices and style guidelines (PEP8 for Python, etc.). Make sure the final output includes a **file tree** of the project and all the files with their content (backend, frontend, infra, tests, docs, CI) in an organized manner. 

(This prompt will instruct the AI to generate a complete External Penetration Test project codebase filling all identified gaps.)

## 2. Internal Network Penetration Test (Red Team)

**Gaps:** No functioning application exists for this project – currently we only have a narrative of an internal pentest (attack paths, etc.) and some placeholder text. The project lacks backend code for simulating or documenting lateral movement and internal recon, lacks a database to store identified internal assets or credentials, and has no UI to visualize attack paths. There are no tests or CI setup. Documentation is limited to a draft report. We need to implement a full-stack solution that captures internal network pentest details, plus add missing documentation and tests.

**AI Prompt:**

You are a Senior Full Stack Engineer with cybersecurity expertise. Build the **Portfolio Project - Internal Network Penetration Test for a Healthcare Organization** as a complete, production-ready application.

**Requirements:**
- Develop a FastAPI **backend** that models an internal corporate network pentest scenario. Include models for **Systems/Hosts** (e.g., workstations, servers identified by hostname/IP), **Findings** (vulnerabilities or misconfigurations found on those systems), and **AttackPaths** (sequences of steps demonstrating lateral movement or privilege escalation). For example, an AttackPath could link multiple hosts and credentials (like “Compromise Host A → extract credentials → move to Host B”). Provide CRUD API endpoints for Systems and Findings, and a custom endpoint to **document an attack path** (e.g., `POST /attack-paths` with a list of steps linking systems and the method used at each step). This endpoint should verify that the systems referenced exist and then store the path. Implement authentication (JWT) for modifying data. Include the standard `/health` endpoint for health checks.
- The backend should also include some **analysis utility** (as an internal function or `/analyze` endpoint) that, given a compromised host, tries to suggest possible lateral movement (for demo purposes, you can implement a simple rule: e.g., if a host has stolen credentials, find another host where those credentials might be reused). This simulates the process of identifying next targets.
- Create a React **frontend** that allows a pentester to view the network map and findings. Provide a page listing all Hosts in the network with basic info (OS, IP, criticality). Provide another page or section showing **Attack Paths** discovered: e.g., a step-by-step list (“Phishing -> Workstation1 -> stolen creds -> Domain Controller”). This can be a simple textual list or a basic directed graph visualization (optional: you can use a library or ASCII diagram for simplicity). The UI should allow adding a new Finding to a Host via a form, and creating an Attack Path by selecting sequence of hosts and describing the technique (e.g., “used Mimikatz on HostA to get creds, then Pass-the-Hash to HostB”). All API calls from the frontend (for adding data or fetching lists) should handle auth (assume a JWT stored in localStorage after a login).
- **Testing:** Write unit tests for backend logic (especially the lateral movement suggestion function and any complex data linking in AttackPaths). Include at least one test for the AttackPath creation API ensuring it fails gracefully if a referenced host ID does not exist (to demonstrate input validation). On the frontend, include a test for the Attack Path form component logic (e.g., ensuring it collects steps correctly). 
- **Infrastructure & Deployment:** Write a Terraform script in `infra/` to deploy this service (similar to Project 1, with an EC2 or ECS for FastAPI, an RDS for Postgres, etc.). This can reuse patterns from the previous project (VPC, subnets, security groups), adjusting resource names for this “internal-pentest” service. Include output of important endpoints or DNS names. Ensure Terraform variables and state config are handled (with an S3 backend or local backend configured as needed). 
- Provide a Docker setup with separate Dockerfiles for backend and frontend. Ensure the backend Dockerfile uses gunicorn/uvicorn to serve the FastAPI app. The docker-compose should allow running the stack locally (backend, db, frontend).
- **CI/CD:** Implement a GitHub Actions workflow that runs backend tests (with a Postgres service for integration tests if needed), runs frontend build and tests, lints both codebases, and builds images. Use matrix jobs or sequential steps as appropriate, and ensure the workflow is deterministic with pinned action versions.
- **Documentation:** Create a comprehensive `README.md` in the project directory (`projects/internal-net-pentest/README.md`). Explain the scenario this project simulates (internal pentest focusing on lateral movement in a healthcare org). Include setup instructions (similar to project 1: how to configure environment variables, run migrations, start the app, run tests). Document how to use the application: e.g., “Add hosts, then add findings, then create an attack path linking them.” Provide an example scenario in the documentation (for example, outline a sample attack path from a low-privileged host to domain admin and how to record it in the system). Also mention any security considerations in the implementation (for instance, note that this is a simulation and does not actually execute real attacks).
- **Explanations in Code:** Comment the code extensively. For example, in the route handling AttackPath creation, add comments explaining how we validate host IDs and why that’s important. In the analysis utility, comment on what the logic is simulating (e.g., “# In a real scenario, this might query Active Directory or use BloodHound to find attack paths. Here we simply look for hosts sharing credentials as a demo.”). The goal is that someone reading the code or the generated docs can learn how an internal pentest is structured technically.
- Follow the same conventions as other projects: consistent naming, pinned dependency versions, and include the `/health` check route. Ensure all generated file paths and references are correct (no placeholder that isn’t replaced).

Output a structured solution with a file tree and file contents for all components (backend, frontend, infra, tests, docs, CI). The output should be ready to run.

(This prompt will generate a full implementation of the Internal Network Penetration Test project, adding all missing code, infrastructure, and documentation.)

## 3. Web Application Security Assessment (Red Team)

**Gaps:** This project currently has only a synopsis of an OWASP Top 10 assessment on an e-commerce platform, but no actual tool or code. Missing components include a web scanner or at least a system to record test cases and results, any form of backend service or database, and a UI for visualizing vulnerabilities (like SQL injection findings). There are placeholders in docs referencing OWASP risks, but no concrete implementation. No CI, no tests, and no deployment scripts exist. We need to create a tool that demonstrates a web app security test environment, with full-stack code and documentation.

**AI Prompt:**

Act as a Full-Stack Security Developer. Construct the **Portfolio Project - Web Application Security Assessment (E-commerce Platform)** with all required components.

**Features to implement:**
- A FastAPI **backend** simulating a web vulnerability assessment tool for an e-commerce web application. Create data models for **Pages/Endpoints** (web pages or API endpoints of the e-commerce app under test) and **VulnFindings** (security issues like XSS, SQL injection, etc.). The API should allow adding a new Page (with URL or name) and recording one or more findings for that page. For example, one finding might be: Page = “/login”, Vulnerability = “SQL Injection”, Details = "able to bypass login with ' OR 1=1 --". Include fields like severity (e.g., High, Medium) and an OWASP Top 10 category tag for each finding.
- Implement an endpoint `POST /scan-page` that, given a page URL and some input parameters, will **simulate scanning** that page for vulnerabilities. Since we can't run a real scanner here, implement a dummy function that checks the URL string for certain patterns and returns a faux vulnerability: e.g., if the page URL or content contains “id=”, pretend to find an “SQL Injection” issue; if it contains “<script>” or similar, find an “XSS” issue. These rules can be simple and clearly commented as placeholder logic. The endpoint should then create corresponding VulnFindings in the database for that page. (This is to demonstrate automation in the assessment.)
- The backend requires auth (JWT) for modifying data (adding pages or running scans). Also include a `/health` endpoint for uptime checks.
- Develop a React **frontend** for security testers to use this tool. The UI should have:
  - A page listing all discovered vulnerabilities grouped by Page/Endpoint. Each Page entry could show a summary (count of vulns, highest severity) and expand to show detailed findings.
  - A form or interface to trigger a **Scan** of a given page: The user inputs a page URL and perhaps selects a type of scan (e.g., “SQLi detection” or “XSS detection” – you can simulate these options in the dummy logic). When submitted, the app calls the backend scan endpoint and then updates the list of findings.
  - An authentication mechanism for the tester to login (username/password hard-coded is fine for demo). Use a context or Redux to store auth state; show a login page and protect the main app pages unless logged in.
  - Use a responsive design (with a CSS framework or custom styles) so the vulnerability list is easy to read.
- **Documentation & Explanation:** In the code, comment on how the dummy scanning works (e.g., “# NOTE: This is a simplified logic to mimic vulnerability scanning for demo purposes”). In the **README.md** (`projects/web-app-assessment/README.md`), explain the concept: this tool does not test a real application but demonstrates how a web security assessment might be cataloged. Include instructions to run the tool, and an example usage scenario (for instance: “Scan the `/product?id=123` page, the tool will report a sample SQL Injection vulnerability”). Also mention that in a real-world assessment, tools like Burp Suite or ZAP would be used – our implementation is for portfolio demonstration only.
- **Testing:** Write unit tests for the `scan_page` logic (e.g., test that a URL containing certain patterns yields the expected dummy vulnerability). Also test the creation of vulnerabilities via the API (ensuring a new finding is correctly linked to a page). For the frontend, a simple test could verify that the vulnerability list component correctly renders a given set of findings (you can mock the API response).
- **Infrastructure:** Provide Terraform config to deploy the app (perhaps similar to earlier projects – reuse the network module, set up an ECS service or an EC2 instance for the backend, and an S3+CloudFront for the static frontend). If time allows, include a security group that only allows the tester’s IP (as a pretend scenario of restricted access, since this is a sensitive tool).
- **CI/CD:** Include a GitHub Actions workflow that lints and tests the backend and frontend, and builds/deploys if this were real (the deploy step can just echo that it would deploy to AWS, since actual deployment might be out-of-scope). Ensure this pipeline uses pinned versions and is consistent with other projects.
- **Containerization:** Dockerize the backend and frontend as usual, so the tool can run via `docker-compose up` for local testing. For example, the backend container might seed the database with one sample page and vulnerability if run with an env like `DEMO_MODE=1`.
- Use pinned dependency versions and consistent naming across components. Verify that the file references (e.g., in README or CI badges) are correct and nothing is left as a placeholder.
- Finally, output the **complete file tree and file contents** for this project, similar to previous ones, ensuring that all missing pieces are now implemented.

Make sure to provide thorough comments and a clear README so that the purpose and usage of this Web App Security Assessment tool are fully understandable to reviewers.

(This prompt directs the AI to generate a working web app security assessment tool with all previously missing elements, including scanning simulation and documentation.)

## 4. Multi-Vector Social Engineering Campaign (Red Team)

**Gaps:** The project content so far is mostly descriptive (phishing, vishing, physical tests) with no code or system to represent it. There is no application to simulate or track a social engineering campaign, and no environment for demonstrating multi-vector attacks. We need to create an application (likely mostly a reporting/tracking tool) with some automation for metrics (like click rates) and possibly an email simulation component. Absent components include backend, frontend, data model for campaign results, test coverage, CI, and infra.

**AI Prompt:**

You are a Software Engineer specializing in security automation. Develop the **Portfolio Project - Multi-Vector Social Engineering Campaign Assessment** as a full-stack application with all supporting elements.

**Objectives:**
- Create a FastAPI **backend** to plan, execute (simulate), and track a multi-vector social engineering campaign. The campaign can have multiple components: **phishing emails, vishing calls, physical intrusion tests,** etc. Design a data model with tables for **Campaign** (overall campaign info like target organization, start/end dates), **PhishingResult** (records of phishing email targets and whether they clicked or reported), **VishingResult** (call attempt outcomes), and **PhysicalTestResult** (entries for physical security tests like tailgating attempts outcome). We won’t integrate with actual email servers or phones – instead, include an endpoint to **simulate campaign execution**: e.g., `POST /campaigns/{id}/execute` which will iterate through all targets (we can store targets as part of Campaign data) and randomly mark outcomes (like 30% of phishing targets clicked the link, etc.). Use deterministic randomness (a fixed seed) so results are reproducible for demonstration.
- Provide CRUD APIs: one to create a new Campaign with lists of targets for each vector (e.g., list of employee emails for phishing, list of phone numbers for vishing), and endpoints to fetch the results. When a campaign is executed via the API, generate results for each target (store whether each target “clicked” or “caught” the bait). Also have an endpoint `GET /campaigns/{id}/report` that aggregates results (e.g., total phishing emails sent, % clicked, % reported, etc.) and returns those metrics.
- Implement authentication (JWT) for any modification actions (creating campaigns, executing them), but allow fetching the report without auth (so stakeholders can view results).
- **Frontend:** Build a React application that allows a user (the social engineering tester) to configure and view campaigns. Key UI pages:
  - Campaign Creation Form: input campaign name, description, and lists of targets for each attack vector (maybe as comma-separated text areas for emails, phone numbers, etc.).
  - Campaign Dashboard: after creation, show a summary of that campaign. Include a button “Execute Campaign” that calls the API to simulate sending phishing emails/calls/etc. Then display results: use charts or at least numeric highlights for each vector (e.g., “Phishing: 5/20 clicked (25%), 2 reported; Vishing: 3/10 divulged info (30%)”, etc.). A simple way is to use a library like Chart.js or just HTML/CSS bars to represent percentages.
  - All Campaigns Page: list campaigns with their high-level outcomes (so the user can select one to view details).
  - The UI should be intuitive and make it clear that this is a simulation. Perhaps use icons or color-coding for success (user resisted) vs failure (user fell victim).
- **Testing:** For the backend, write tests for the campaign execution logic to ensure the statistics generation is correct (for a given random seed and inputs, the outcomes count matches expected percentages or distribution). For example, if our simulation logic says “randomly 20% click rate”, test that roughly that number is marked in results. Also test the report aggregation function. On the frontend, you might include a test for the report component to ensure it correctly interprets an API response (maybe mock a sample report JSON and test rendering).
- **CI/CD:** Provide a GitHub Actions workflow to run backend tests, lint code, build the frontend, and (optionally) run a minimal end-to-end test (for instance, using `pytest` with `requests` to spin up the FastAPI app and hit the `/health` and one campaign endpoint). Ensure to use services in GH Actions if needed (like Postgres for DB during tests). Use caching for dependencies to speed up, and pin action versions.
- **Infrastructure as Code:** Write Terraform for deploying this service (campaign tracker) – including perhaps an AWS Lambda or EC2 for the backend (or even AWS Fargate service), an S3 for static frontend, and a DynamoDB or RDS depending on preference (to demonstrate multi-tech, maybe try using DynamoDB for this one to vary the stack, unless sticking to Postgres for simplicity – you can decide a reasonable approach and document it). If using DynamoDB (NoSQL) for results, adjust data model accordingly; if using RDS, stick to that. The Terraform should also create IAM roles or any needed resource (if using email simulation, maybe nothing external needed).
- **Documentation:** Prepare a `README.md` (`projects/social-engineering-campaign/README.md`). It should explain what vectors are included (phishing, vishing, physical) and that all results are simulated. Include how to run the simulation: e.g., “Create a campaign with X targets and hit Execute.” Document the expected outcomes (maybe note that random outcomes are generated and what the default rates are). Also include a note that **no real emails or calls are made**. Provide deployment instructions using the Terraform scripts. If possible, include a small screenshot or ASCII table of sample results in the README for clarity.
- **Comments & Explanations:** In-code comments should clarify simulation logic. E.g., in the function that generates phishing outcomes, comment on how in a real scenario this might integrate with an email platform, but here we randomize results. Also comment any security considerations (like we ensure not to store real credentials, etc.). The goal is clarity for someone reviewing the code who might not know simulation vs real.
- Maintain consistency in style and ensure all references (like campaign ID in results) are correctly handled. Include the `/health` endpoint for completeness. 

Finally, output the full project structure with all files and content, making sure it’s a complete, working implementation.

(This prompt guides the AI to create a social engineering campaign management and simulation tool, covering the previously missing application logic, UI, and support systems.)
