# Kuiper System/Network Administrator Interview Cheat Sheet
## Q11-Q20: Connectivity & Traffic Engineering

---

### Q11: Choosing Direct Connect vs VPN

| Column | Content |
|--------|---------|
| **Question/Topic** | When should you use Direct Connect vs Site-to-Site VPN? |
| **Core Answer** | **Direct Connect (DX):**<br>‚úÖ Dedicated private connection (1, 10, 100 Gbps ports)<br>‚úÖ Consistent, predictable bandwidth and latency<br>‚úÖ Lower per-GB costs for high-volume transfer (>5-10TB/month)<br>‚úÖ Private peering with AWS (doesn't traverse public internet)<br>‚ùå Not encrypted by default (need VPN overlay or MACsec)<br>‚ùå Longer setup time (weeks to provision)<br>‚ùå Higher monthly fixed cost<br><br>**Site-to-Site VPN:**<br>‚úÖ Encrypted by default (IPsec)<br>‚úÖ Fast setup (minutes to hours)<br>‚úÖ Low monthly cost (pay for tunnel hours + data transfer)<br>‚úÖ Flexible routing with BGP<br>‚ùå Variable performance (uses public internet)<br>‚ùå Lower bandwidth limits (~1.25 Gbps per tunnel, ~2.5 Gbps ECMP)<br>‚ùå Higher per-GB costs<br><br>**Best Practice:** Use DX as primary for high-bandwidth stable traffic + VPN as encrypted backup with BGP traffic engineering. |
| **Feynman Explanation** | Direct Connect is like building a private highway from your house to AWS‚Äîcosts more up front, but it's always fast, never congested, and only you use it. VPN is like driving on public roads through a secret tunnel‚Äîit's cheaper to start using, but speed varies with traffic, and there's a speed limit. Most big companies use both: private highway for everyday driving (big data transfers), public roads + tunnel as backup when the highway has construction. |
| **All Acronyms Explained** | ‚Ä¢ **DX** = Direct Connect<br>‚Ä¢ **VPN** = Virtual Private Network<br>‚Ä¢ **Gbps** = Gigabits per second<br>‚Ä¢ **IPsec** = Internet Protocol Security (VPN encryption)<br>‚Ä¢ **MACsec** = Media Access Control Security (Layer 2 encryption for DX)<br>‚Ä¢ **TB** = Terabyte (1,000 GB)<br>‚Ä¢ **ECMP** = Equal-Cost Multi-Path<br>‚Ä¢ **BGP** = Border Gateway Protocol<br>‚Ä¢ **TE** = Traffic Engineering (controlling traffic flow)<br>‚Ä¢ **MTU** = Maximum Transmission Unit (largest packet size, 1500 standard, 9001 jumbo frames) |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p03-hybrid-network/README.md` - Describes VPN implementation<br><br>**Create ADR:** `docs/adr/ADR-0003-dx-vs-vpn-kuiper-gateway.md`<br>```<br>Context: Kuiper ground gateway needs 10 Gbps to AWS for user traffic<br>Options:<br>1. VPN only: Max 2.5 Gbps, $0.09/GB, $87.60/tunnel-month<br>2. DX only: 10 Gbps, $0.02/GB, $1,620/month port + $220/month AWS<br>3. DX primary + VPN backup: Best of both<br><br>Calculation (500 TB/month):<br>- VPN only: (500,000 GB √ó $0.09) + (2 tunnels √ó $87.60) = $45,175/mo ‚ùå<br>- DX only: (500,000 GB √ó $0.02) + $1,620 + $220 = $11,840/mo ‚úÖ<br>- DX+VPN: $11,840 + $175 (VPN standby) = $12,015/mo ‚úÖ RESILIENT<br><br>Decision: Direct Connect 10G primary + Site-to-Site VPN backup<br>```<br><br>**Add Cost Table:** `docs/cost-analysis/dx-vs-vpn.xlsx` |
| **Common Pitfalls to Avoid** | ‚ùå Using DX without VPN backup (no redundancy, single point of failure)<br>‚ùå Not encrypting Direct Connect (compliance violation for sensitive data)<br>‚ùå Choosing VPN for high-volume (>5 TB/month) without cost analysis<br>‚ùå Assuming DX is always faster (latency is similar if AWS region is far from DX location)<br>‚ùå Forgetting DX provisioning time (can't be spun up in emergency)<br>‚ùå Not testing failover from DX to VPN (might not work when needed!)<br>‚ùå Mismatched MTU sizes (DX supports 9001 jumbo frames, VPN typically 1400) |
| **Risk Level** | üî¥ **HIGH** - Critical architectural decision, shows cost and technical analysis skills |
| **Time to Learn** | ‚è±Ô∏è **4-5 hours** - Study AWS DX docs, pricing calculator, BGP for TE |
| **Owner/Accountability** | üë§ **You** - Prepare cost analysis and decision rationale |

---

### Q12: Landing Two Kuiper Gateways into AWS

| Column | Content |
|--------|---------|
| **Question/Topic** | How would you architect connectivity from two Kuiper ground gateways into AWS? |
| **Core Answer** | **Architecture:**<br>1. **Physical:** Two separate gateway POPs in different cities/ISPs for geographic diversity<br>2. **AWS Connectivity:** Each gateway has DX primary + VPN backup to same regional TGW (4 connections total)<br>3. **BGP Traffic Engineering:** Use BGP communities and local-preference to control path preference:<br>   - Normal: Both gateways active (50/50 split or 70/30 based on capacity)<br>   - Gateway A failure: Gateway B takes 100% (automatic via BGP)<br>4. **User Steering:** Route 53 latency-based or geolocation routing with health checks points users to nearest healthy gateway<br>5. **Monitoring:** CloudWatch alarms on tunnel state, BGP sessions, gateway metrics; synthetic probes from multiple regions<br>6. **Failover Time:** BGP convergence (30-60s) + DNS TTL (60s) = ~2 min total failover<br><br>**Key:** Test failover monthly, document runbooks, alert NOC immediately on gateway degradation. |
| **Feynman Explanation** | You build two separate space signal receiving stations in different cities (in case an earthquake hits one). Each station has a private highway (Direct Connect) and a backup tunnel road (VPN) to Amazon's cloud. You use smart traffic signs (BGP routing) that automatically redirect all space traffic to the healthy station if one breaks. You also use GPS navigation (Route 53 DNS) to send customers to the station closest to them. This way, even if a whole city loses power, the satellite internet keeps working from the other station! |
| **All Acronyms Explained** | ‚Ä¢ **POP** = Point of Presence (physical facility with network equipment)<br>‚Ä¢ **DX** = Direct Connect<br>‚Ä¢ **VPN** = Virtual Private Network<br>‚Ä¢ **TGW** = Transit Gateway<br>‚Ä¢ **BGP** = Border Gateway Protocol<br>‚Ä¢ **TE** = Traffic Engineering<br>‚Ä¢ **Communities** = BGP tags for route signaling (32-bit values like 65000:100)<br>‚Ä¢ **Local-preference** = BGP attribute to prefer routes within your network (higher = better)<br>‚Ä¢ **Route 53** = AWS DNS service<br>‚Ä¢ **DNS** = Domain Name System<br>‚Ä¢ **TTL** = Time To Live (how long to cache DNS answer, in seconds)<br>‚Ä¢ **NOC** = Network Operations Center |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p10-multi-region/README.md` - Shows multi-site routing and failover<br><br>**Create New Project Evidence:**<br>1. **Diagram:** `docs/diagrams/kuiper/dual-gateway-architecture.mmd` (Mermaid diagram)<br>   - Show 2 gateways, 2 DX, 2 VPN, 1 TGW, Route 53, user traffic flow<br>2. **Terraform:** `terraform/kuiper-dual-gateway/`<br>   - `gw-a.tf` (CGW A, VPN A, DX A attachment to TGW)<br>   - `gw-b.tf` (CGW B, VPN B, DX B attachment to TGW)<br>   - `tgw.tf` (Transit Gateway, route tables, BGP ASN config)<br>   - `route53.tf` (Latency routing policy, health checks for both gateways)<br>3. **Runbook:** `docs/runbooks/kuiper-gateway-failover.md`<br>   - Detection, escalation, failover steps, rollback<br>4. **Test Results:** `evidence/chaos-tests/gateway-a-failure/`<br>   - Screenshots of BGP routes before/during/after<br>   - Route 53 query log showing DNS switch<br>   - User-facing latency impact graph |
| **Common Pitfalls to Avoid** | ‚ùå Single ISP for both gateways (correlated failure risk)<br>‚ùå Not testing failover regularly (it won't work when needed)<br>‚ùå BGP mis configuration causing traffic black hole during failover<br>‚ùå DNS TTL too long (slow failover) or too short (excessive DNS load)<br>‚ùå No health checks‚Äîsending users to dead gateway via DNS<br>‚ùå Asymmetric routing (traffic in via Gateway A, out via Gateway B‚Äîbreaks stateful firewalls)<br>‚ùå Not monitoring both gateways equally (blind spot in secondary)<br>‚ùå Manual failover process (too slow, error-prone) |
| **Risk Level** | üî¥ **CRITICAL** - This is the core architecture for Kuiper ground operations |
| **Time to Learn** | ‚è±Ô∏è **6-8 hours** - Design architecture, create diagrams, write Terraform, document runbook |
| **Owner/Accountability** | üë§ **You** - This should be your showcase piece for the interview |

---

### Q13: Multi-Region Scaling

| Column | Content |
|--------|---------|
| **Question/Topic** | How would you scale Kuiper gateway infrastructure across multiple AWS regions? |
| **Core Answer** | **Pattern:**<br>1. **Regional TGWs:** Deploy one Transit Gateway per region (e.g., us-east-1, us-west-2, eu-west-1)<br>2. **Local Gateways:** Each region has 2+ ground gateways (local redundancy) connecting to regional TGW<br>3. **Inter-Region Links:** Use TGW Peering to connect regions (for cross-region traffic if needed)<br>4. **User Steering:** Route 53 with Latency or Geolocation policies sends users to nearest region<br>5. **Control Plane:** Keep regional (each region's Prometheus, Grafana, alerting‚Äîavoid cross-region dependencies)<br>6. **Data Plane:** Localized (user data stays in-region for latency; only metadata/control sync cross-region)<br>7. **Failure Isolation:** Region failure doesn't cascade‚Äîother regions continue operating<br>8. **Test:** Regularly simulate full region failure (Game Days) |
| **Feynman Explanation** | Think of each AWS region as a separate country with its own post office system (Transit Gateway) and its own mail carriers (ground gateways). Normally, letters stay within the country for speed. But you build bridges (TGW Peering) between countries for special deliveries. A smart address system (Route 53) automatically sends your letter to the nearest post office. If one country's post office burns down, all other countries keep running‚Äîthat's the magic of regional isolation! |
| **All Acronyms Explained** | ‚Ä¢ **TGW** = Transit Gateway<br>‚Ä¢ **AWS** = Amazon Web Services<br>‚Ä¢ **Region** = Geographic area with multiple availability zones (e.g., us-east-1 is N. Virginia)<br>‚Ä¢ **TGW Peering** = Direct connection between TGWs in different regions<br>‚Ä¢ **Route 53** = AWS DNS service<br>‚Ä¢ **Latency-based routing** = DNS routes to lowest-latency endpoint for user<br>‚Ä¢ **Geolocation routing** = DNS routes based on user's geographic location<br>‚Ä¢ **Control plane** = System management traffic (monitoring, config, APIs)<br>‚Ä¢ **Data plane** = User traffic (actual data being transferred)<br>‚Ä¢ **Game Day** = Scheduled failure simulation exercise<br>‚Ä¢ **Cascading failure** = One failure causing others (domino effect) |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p10-multi-region/README.md` - Your multi-region architecture project<br><br>**Enhancements to Add:**<br>1. **Diagram:** `docs/diagrams/kuiper/multi-region-tgw.mmd`<br>   - 3 regions, each with TGW + 2 gateways<br>   - TGW peering between regions (mesh or hub-spoke)<br>   - Route 53 global routing layer<br>2. **Terraform:** `terraform/kuiper-multi-region/`<br>   - `us-east-1/` (TGW, gateways, VPN)<br>   - `us-west-2/` (TGW, gateways, VPN)<br>   - `eu-west-1/` (TGW, gateways, VPN)<br>   - `global/` (Route 53, TGW peering, IAM roles)<br>3. **Runbook:** `docs/runbooks/region-failover-drill.md`<br>   - Pre-drill checklist, steps to isolate region, validation tests, rollback<br>4. **SLO Doc:** `docs/slo/multi-region-availability.md`<br>   - Target: 99.99% global availability (52 min downtime/year)<br>   - Allows 1 region failure without impact |
| **Common Pitfalls to Avoid** | ‚ùå Shared control plane across regions (region failure takes down monitoring everywhere)<br>‚ùå Synchronous cross-region database writes (latency kills performance)<br>‚ùå Not considering data sovereignty/residency laws (GDPR, etc.)<br>‚ùå Underestimating TGW peering costs (cross-region data transfer is expensive)<br>‚ùå Assuming TGW peering is instant (has bandwidth limits, routing propagation delay)<br>‚ùå Never testing region failover (will fail during real outage)<br>‚ùå Complex dependencies between regions (tight coupling prevents scaling)<br>‚ùå Same-region backup (defeats purpose of multi-region) |
| **Risk Level** | üî¥ **HIGH** - Shows understanding of global-scale architecture |
| **Time to Learn** | ‚è±Ô∏è **5-6 hours** - Study multi-region patterns, AWS regional services, disaster recovery |
| **Owner/Accountability** | üë§ **You** - Be ready to discuss regional isolation and failover strategies |

---

### Q14: Safe Change Plans (Network Changes)

| Column | Content |
|--------|---------|
| **Question/Topic** | What should a safe network change plan include? |
| **Core Answer** | **1. Pre-Change:**<br>‚Ä¢ Detailed description of change (what, why, impact)<br>‚Ä¢ Change window (date/time, duration)<br>‚Ä¢ Blast radius analysis (what could break?)<br>‚Ä¢ Current state snapshot (dashboards, route tables, configs)<br>‚Ä¢ Peer review of change (minimum 2 engineers)<br>‚Ä¢ Rollback plan (specific commands/actions)<br>‚Ä¢ Communication plan (notify stakeholders, NOC, on-call)<br><br>**2. During Change:**<br>‚Ä¢ Follow runbook step-by-step (don't improvise)<br>‚Ä¢ Use staging environment first if possible<br>‚Ä¢ Apply changes incrementally (test after each step)<br>‚Ä¢ Monitor dashboards continuously<br>‚Ä¢ Run synthetic probes to validate connectivity<br>‚Ä¢ Document any deviations<br><br>**3. Post-Change:**<br>‚Ä¢ Verify all monitors green (BGP up, tunnels up, routes correct)<br>‚Ä¢ Run automated tests (ping, traceroute, HTTP probes)<br>‚Ä¢ Compare pre/post metrics (latency, throughput, errors)<br>‚Ä¢ Soak period (monitor for 15-30 min before declaring success)<br>‚Ä¢ Update documentation if procedures changed<br>‚Ä¢ Post-change review (what went well, what could improve) |
| **Feynman Explanation** | Changing network configs is like performing surgery‚Äîyou need a plan! Before you start, take pictures of the patient (snapshot dashboards), tell everyone what you're doing (communication), have a backup plan if things go wrong (rollback), and get a second doctor to review your plan (peer review). During surgery, go slow, check vital signs constantly (monitoring), and follow your checklist exactly. After surgery, watch the patient closely for a while (soak period) to make sure they're really okay before sending them home. Write down what you learned for next time! |
| **All Acronyms Explained** | ‚Ä¢ **NOC** = Network Operations Center<br>‚Ä¢ **BGP** = Border Gateway Protocol<br>‚Ä¢ **VPN** = Virtual Private Network<br>‚Ä¢ **SLA** = Service Level Agreement (contractual uptime promise)<br>‚Ä¢ **SLO** = Service Level Objective (internal target)<br>‚Ä¢ **RCA** = Root Cause Analysis<br>‚Ä¢ **RFO** = Reason for Outage (report)<br>‚Ä¢ **CAB** = Change Advisory Board (approves high-risk changes)<br>‚Ä¢ **MTTR** = Mean Time To Repair (average time to fix issues)<br>‚Ä¢ **Blast radius** = Scope of potential impact from change<br>‚Ä¢ **Soak period** = Monitoring time after change before declaring success<br>‚Ä¢ **Rollback** = Reverting to previous configuration |
| **Practical Example from Your Portfolio** | **Create Change Management Evidence:**<br>1. **Template:** `docs/runbooks/change/network-change-template.md`<br>   - Pre-filled sections for all required fields<br>   - Checklist format for easy following<br>2. **Example Change:** `docs/runbooks/change/examples/2025-01-add-vpn-tunnel-c.md`<br>   - Real example of adding a third VPN tunnel<br>   - Includes screenshots of pre-state, change commands, post-state<br>   - Shows rollback that was prepared (but not needed)<br>3. **Postmortem Template:** `docs/postmortems/template.md`<br>   - For when changes go wrong<br>   - Blameless, focuses on learning<br>4. **Reference Your Homelab:**<br>   - `projects/06-homelab/PRJ-HOME-002/changes/` - Log of changes you made<br>   - Shows discipline even in personal lab |
| **Common Pitfalls to Avoid** | ‚ùå "Cowboy changes" (no plan, no review, "I'll just fix it quickly")<br>‚ùå Making changes during peak business hours (maximize blast radius!)<br>‚ùå No rollback plan ("we'll figure it out if it breaks")<br>‚ùå Not testing in lab/staging first<br>‚ùå Making multiple changes at once (can't isolate what broke)<br>‚ùå Declaring success immediately (issues often appear 10-15 min later)<br>‚ùå Not documenting actual steps taken (can't repeat or troubleshoot)<br>‚ùå Skipping post-change review (miss chance to improve process)<br>‚ùå Not communicating to stakeholders (angry customers/managers) |
| **Risk Level** | üî¥ **CRITICAL** - Change management discipline prevents outages |
| **Time to Learn** | ‚è±Ô∏è **3-4 hours** - Study ITIL change management, create templates, review incident reports |
| **Owner/Accountability** | üë§ **You** - Demonstrate mature change management practices |

---

### Q15: Route 53 Latency/Geo Steering

| Column | Content |
|--------|---------|
| **Question/Topic** | How do you use Route 53 to steer users to the best gateway/endpoint? |
| **Core Answer** | **Latency-Based Routing:**<br>‚Ä¢ Create multiple A/AAAA records with same name, different IPs (one per gateway/region)<br>‚Ä¢ Set routing policy to "Latency"<br>‚Ä¢ Route 53 uses latency measurements between user location and AWS regions to pick lowest-latency endpoint<br>‚Ä¢ Best for performance optimization when your endpoints are in AWS regions<br><br>**Geolocation Routing:**<br>‚Ä¢ Map specific geographies (continent/country/state) to specific endpoints<br>‚Ä¢ Example: EU users ‚Üí eu-west-1 gateway, US users ‚Üí us-east-1 gateway<br>‚Ä¢ Best for data sovereignty, compliance, or when you want deterministic routing<br>‚Ä¢ Requires "default" location for users not matching any rule<br><br>**Geoproximity Routing:**<br>‚Ä¢ Routes based on distance + bias value (shift traffic toward/away from location)<br>‚Ä¢ Useful for load balancing or controlled migration<br><br>**Key:** Combine with health checks, use low TTL (60s) for fast failover, and monitor Route 53 query logs. |
| **Feynman Explanation** | Route 53 is like a smart traffic cop for the internet. Latency routing is like the cop measuring how long it takes to get to each destination and sending you the fastest way‚Äîeven if it changes hour to hour. Geolocation routing is like the cop saying "all people from California go left, all people from New York go right"‚Äîbased on where you're from, not speed. You combine this with health checks so the cop never sends you down a closed road (broken gateway). Change the signs fast (low TTL) so if a road closes, new drivers know immediately. |
| **All Acronyms Explained** | ‚Ä¢ **Route 53** = AWS's DNS service (named after DNS port 53)<br>‚Ä¢ **DNS** = Domain Name System (converts names to IP addresses)<br>‚Ä¢ **A record** = DNS record mapping name to IPv4 address<br>‚Ä¢ **AAAA record** = DNS record mapping name to IPv6 address (quad-A)<br>‚Ä¢ **TTL** = Time To Live (seconds to cache DNS answer)<br>‚Ä¢ **Latency** = Network delay (time for data to travel)<br>‚Ä¢ **Geolocation** = Geographic location of user<br>‚Ä¢ **Geoproximity** = Distance-based routing with bias<br>‚Ä¢ **Health check** = Automated probe to verify endpoint is healthy<br>‚Ä¢ **Failover** = Switching to backup when primary fails<br>‚Ä¢ **GDPR** = General Data Protection Regulation (EU privacy law) |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p10-multi-region/README.md` - Your multi-region project likely uses Route 53<br><br>**Enhancements:**<br>1. **Terraform:** `terraform/kuiper-route53-latency/`<br>   ```hcl<br>   resource "aws_route53_record" "gateway_latency" {<br>     for_each = {<br>       us_east_1 = "1.2.3.4"<br>       us_west_2 = "5.6.7.8"<br>       eu_west_1 = "9.10.11.12"<br>     }<br>     zone_id        = aws_route53_zone.main.zone_id<br>     name           = "gateway.kuiper.example.com"<br>     type           = "A"<br>     set_identifier = each.key<br>     latency_routing_policy {<br>       region = each.key<br>     }<br>     ttl     = 60<br>     records = [each.value]<br>     health_check_id = aws_route53_health_check.gateway[each.key].id<br>   }<br>   ```<br>2. **Dashboard:** `dashboards/cloudwatch-route53.json`<br>   - Query count per routing policy decision<br>   - Health check status<br>   - Query latency<br>3. **Test Evidence:** `evidence/dns-tests/`<br>   - Run `dig` from different global locations (use public DNS resolvers)<br>   - Show different IP returned based on latency |
| **Common Pitfalls to Avoid** | ‚ùå High TTL (300s+) makes failover slow‚Äîusers cache dead IP<br>‚ùå Not setting up health checks‚ÄîRoute 53 sends users to dead gateway<br>‚ùå Assuming latency routing is perfect‚Äîit's based on AWS's latency data, not perfect real-time<br>‚ùå Forgetting "default" geolocation rule‚Äîusers from unexpected locations get NXDOMAIN error<br>‚ùå Not monitoring Route 53 query logs‚Äîyou're blind to routing decisions<br>‚ùå Mixing routing policies on same record (not allowed, must choose one)<br>‚ùå Thinking Route 53 checks application health‚Äîit only checks endpoint reachability unless you configure detailed checks |
| **Risk Level** | üü° **MEDIUM-HIGH** - Important for global traffic steering |
| **Time to Learn** | ‚è±Ô∏è **3-4 hours** - Study Route 53 routing policies, create examples, test from different locations |
| **Owner/Accountability** | üë§ **You** - Be able to explain when to use each routing policy |

---

### Q16: Route 53 Health Checks for Failover

| Column | Content |
|--------|---------|
| **Question/Topic** | How do Route 53 health checks enable automatic failover? |
| **Core Answer** | **Setup:**<br>1. Create health check resource for each endpoint<br>2. Configure check type: HTTP/HTTPS (checks specific path, expects 200-399 status), TCP (checks port open), or calculated (combines multiple checks)<br>3. Set check interval: 30s (standard) or 10s (fast, higher cost)<br>4. Set failure threshold: e.g., 3 consecutive failures = unhealthy<br>5. Enable health checker regions: checks from multiple AWS regions (global validation)<br>6. Associate health check with Route 53 DNS record<br><br>**Behavior:**<br>‚Ä¢ When endpoint healthy: Route 53 includes it in DNS answers<br>‚Ä¢ When endpoint unhealthy (fails threshold): Route 53 removes it from DNS answers within 1 minute (check interval + failure threshold + propagation)<br>‚Ä¢ Automatic recovery: When health check passes again, endpoint automatically added back<br><br>**Monitoring:**<br>‚Ä¢ CloudWatch metrics for each health check (HealthCheckStatus, HealthCheckPercentageHealthy)<br>‚Ä¢ SNS notifications on health status changes<br>‚Ä¢ Combine with low TTL for fast client-side failover |
| **Feynman Explanation** | Health checks are like having doctors continuously checking on your ground gateways. Every 30 seconds, doctors from around the world (multiple AWS regions) call the gateway and ask "are you okay?" If the gateway doesn't answer correctly 3 times in a row, the doctors declare it sick. Once declared sick, Route 53 stops telling users to go there‚Äîlike putting an "Out of Order" sign on a broken vending machine. When the gateway answers correctly again, the sign comes off automatically. This all happens without you pressing any buttons! |
| **All Acronyms Explained** | ‚Ä¢ **Route 53** = AWS DNS service<br>‚Ä¢ **HTTP** = Hypertext Transfer Protocol<br>‚Ä¢ **HTTPS** = HTTP Secure (encrypted)<br>‚Ä¢ **TCP** = Transmission Control Protocol<br>‚Ä¢ **DNS** = Domain Name System<br>‚Ä¢ **TTL** = Time To Live (DNS cache duration)<br>‚Ä¢ **SNS** = Simple Notification Service (AWS's pub/sub messaging)<br>‚Ä¢ **CloudWatch** = AWS monitoring service<br>‚Ä¢ **NXDOMAIN** = Non-Existent Domain (DNS error)<br>‚Ä¢ **200-399** = HTTP success status codes (200 = OK, 301 = redirect, etc.)<br>‚Ä¢ **Calculated health check** = Logical combination (AND/OR) of other health checks |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p10-multi-region/README.md`<br><br>**Create Evidence:**<br>1. **Terraform:** `terraform/kuiper-route53-healthchecks/`<br>   ```hcl<br>   resource "aws_route53_health_check" "gateway_us_east" {<br>     fqdn              = "gateway-a.kuiper.internal"<br>     port              = 443<br>     type              = "HTTPS"<br>     resource_path     = "/health"<br>     failure_threshold = 3<br>     request_interval  = 30<br>     measure_latency   = true<br>     regions           = ["us-east-1", "us-west-2", "eu-west-1"]<br>     tags = {<br>       Name = "Gateway US-East Health Check"<br>     }<br>   }<br>   resource "aws_cloudwatch_metric_alarm" "gateway_unhealthy" {<br>     alarm_name          = "gateway-us-east-unhealthy"<br>     comparison_operator = "LessThanThreshold"<br>     evaluation_periods  = 1<br>     metric_name         = "HealthCheckStatus"<br>     namespace           = "AWS/Route53"<br>     period              = 60<br>     statistic           = "Minimum"<br>     threshold           = 1<br>     alarm_actions       = [aws_sns_topic.pagerduty.arn]<br>   }<br>   ```<br>2. **Test:** `docs/runbooks/chaos/r53-health-check-failover-drill.md`<br>   - Step 1: Verify baseline (all health checks green)<br>   - Step 2: Break gateway health endpoint (return 503)<br>   - Step 3: Watch health check fail (capture screenshots)<br>   - Step 4: Query DNS‚Äîverify gateway removed from answers<br>   - Step 5: Fix health endpoint<br>   - Step 6: Watch health check recover<br>   - Step 7: Query DNS‚Äîverify gateway added back<br>3. **Screenshot Evidence:** `evidence/chaos-tests/r53-health-failover/`<br>   - Before: 3 healthy gateways, DNS returns all 3<br>   - During failure: 1 unhealthy, DNS returns only 2<br>   - After recovery: All healthy again |
| **Common Pitfalls to Avoid** | ‚ùå Not implementing a proper health endpoint (checking just HTTP 200 on root isn't enough‚Äîneed to check dependencies)<br>‚ùå Health endpoint not representative of service health (endpoint is up but service is broken)<br>‚ùå Using only one region for health checks (regional AWS issue causes false positives)<br>‚ùå Failure threshold too sensitive (brief blip triggers unnecessary failover)<br>‚ùå No alerting on health check failures (you find out from users)<br>‚ùå Long DNS TTL negates health check benefits (users cache old answers)<br>‚ùå Not testing health check failover (might not work as expected)<br>‚ùå Health endpoint is expensive/slow (health checks hammering it cause issues) |
| **Risk Level** | üî¥ **HIGH** - Critical for automatic failover and high availability |
| **Time to Learn** | ‚è±Ô∏è **4-5 hours** - Study Route 53 health checks, implement in project, run failover drill |
| **Owner/Accountability** | üë§ **You** - Must demonstrate working health check failover |

---

### Q17: Anycast (What & When)

| Column | Content |
|--------|---------|
| **Question/Topic** | Explain anycast and when to use it for Kuiper ground infrastructure |
| **Core Answer** | **What is Anycast:**<br>‚Ä¢ Single IP address announced from multiple locations via BGP<br>‚Ä¢ Internet routing automatically sends users to "nearest" location based on BGP path selection<br>‚Ä¢ All locations respond to same IP‚Äîusers don't know or care which answers<br>‚Ä¢ Works at network layer (Layer 3), requires BGP control<br><br>**When to Use:**<br>‚úÖ Stateless services: DNS, NTP, CDN cache, HTTP API (with session sharing)<br>‚úÖ DDoS mitigation (spread attack across multiple locations)<br>‚úÖ Lowest possible latency (no DNS lookup, direct BGP routing)<br>‚úÖ Large scale (100+ locations)<br><br>**When NOT to Use:**<br>‚ùå Stateful services without global session store (databases, shopping carts, SSH)<br>‚ùå Long-lived TCP connections (BGP changes mid-session break it)<br>‚ùå Sticky sessions/affinity required<br>‚ùå Need observability per-location (anycast complicates source tracking)<br><br>**For Kuiper:** Potentially use anycast for ground gateway DNS resolvers and control-plane APIs (if stateless). NOT for user data plane (sessions too long). |
| **Feynman Explanation** | Anycast is like having five Starbucks all called "Starbucks" with the exact same address. When you put "Starbucks" in your GPS, it automatically sends you to the closest one‚Äîyou don't choose, the GPS chooses for you based on road distance. This is great for quick things like buying a coffee (stateless‚Äîyou don't need to return to the same Starbucks). But it's terrible for things like a multi-day cooking class (stateful‚Äîyou need to return to the same location each day, but anycast might send you to a different Starbucks tomorrow!). |
| **All Acronyms Explained** | ‚Ä¢ **Anycast** = Routing technique where same IP announced from multiple locations<br>‚Ä¢ **BGP** = Border Gateway Protocol (internet routing protocol)<br>‚Ä¢ **DNS** = Domain Name System<br>‚Ä¢ **NTP** = Network Time Protocol (clock synchronization)<br>‚Ä¢ **CDN** = Content Delivery Network<br>‚Ä¢ **API** = Application Programming Interface<br>‚Ä¢ **DDoS** = Distributed Denial of Service (flood attack from many sources)<br>‚Ä¢ **TCP** = Transmission Control Protocol (connection-oriented)<br>‚Ä¢ **SSH** = Secure Shell (remote terminal)<br>‚Ä¢ **Layer 3** = Network layer (IP addresses, routing)<br>‚Ä¢ **Session affinity** = Keeping user connected to same backend<br>‚Ä¢ **Unicast** = Traditional 1-to-1 IP addressing (opposite of anycast) |
| **Practical Example from Your Portfolio** | **Reference:** `projects/05-networking-datacenter/PRJ-NET-DC-001/README.md` - Datacenter networking<br><br>**Create ADR:** `docs/adr/ADR-0004-anycast-vs-dns-routing.md`<br>```markdown<br># ADR-0004: Anycast vs DNS Routing for Kuiper Gateways<br><br>## Context<br>Need to route users to nearest gateway with lowest latency.<br><br>## Options<br>### Option 1: Anycast<br>- Pros: Lowest latency (no DNS lookup), automatic BGP failover<br>- Cons: Breaks long TCP sessions on path change, complex BGP ops<br><br>### Option 2: DNS (Route 53 Latency)<br>- Pros: Handles stateful traffic, easier ops, better observability<br>- Cons: DNS lookup adds latency, TTL limits failover speed<br><br>## Decision<br>Use DNS (Route 53 Latency) for user data plane because:<br>1. User sessions are long-lived (minutes to hours)<br>2. Path stability more important than 20ms DNS lookup<br>3. Need observability per-gateway for SLOs<br>4. Easier operations (no BGP complexity)<br><br>Consider anycast for control-plane DNS resolver and NTP only.<br>```<br><br>**Lab:** `projects/06-homelab/PRJ-HOME-002/anycast-demo/` (if you run BGP in homelab) |
| **Common Pitfalls to Avoid** | ‚ùå Using anycast for everything because "it's faster" (breaks stateful apps)<br>‚ùå Not having graceful drain procedure (route withdrawal causes dropped connections)<br>‚ùå Asymmetric routing (inbound via anycast, return via different path‚Äîbreaks firewalls)<br>‚ùå Not monitoring per-location (anycast IP hides source, complicates troubleshooting)<br>‚ùå BGP mistakes causing black holes (announcing but not actually serving traffic)<br>‚ùå Not considering geopolitical BGP manipulation (ISPs can hijack your anycast prefix)<br>‚ùå Using anycast without understanding BGP (dangerous‚Äîcan break internet routing) |
| **Risk Level** | üü° **MEDIUM** - Good to understand concept and trade-offs |
| **Time to Learn** | ‚è±Ô∏è **3-4 hours** - Study anycast basics, BGP routing, CDN architectures |
| **Owner/Accountability** | üë§ **You** - Be ready to explain when NOT to use anycast (common interview trap) |

---

### Q18: When NOT to Use Anycast

| Column | Content |
|--------|---------|
| **Question/Topic** | When should you avoid anycast routing? |
| **Core Answer** | **Avoid Anycast For:**<br><br>**1. Long-Lived Stateful Connections:**<br>‚Ä¢ Database connections (PostgreSQL, MySQL sessions)<br>‚Ä¢ SSH/RDP sessions (terminal state on server)<br>‚Ä¢ Shopping carts / user sessions without global session store<br>‚Ä¢ VPN tunnels (IPsec state tied to endpoint)<br>‚Ä¢ BGP change mid-session sends traffic to different location = connection break<br><br>**2. Applications Requiring Affinity:**<br>‚Ä¢ Multi-step transactions (begin transaction at location A, commit must be at A)<br>‚Ä¢ Upload/download pairs (upload starts at A, download request goes to B‚Äîfile not there)<br>‚Ä¢ WebSocket connections (need stable endpoint)<br><br>**3. Observability Requirements:**<br>‚Ä¢ Need to identify which location served request (anycast IP same everywhere)<br>‚Ä¢ Per-location SLO tracking (can't distinguish in logs)<br>‚Ä¢ Troubleshooting (hard to know which location had issue)<br><br>**4. Small Scale:**<br>‚Ä¢ Only 2-3 locations (DNS routing simpler, works fine)<br>‚Ä¢ Operational complexity not justified<br><br>**Better Alternative:** Use DNS-based routing (Route 53 Latency/Geo) with session persistence or global session store. |
| **Feynman Explanation** | Remember the Starbucks analogy? Imagine you're working on a 1000-piece puzzle at Starbucks. With anycast, tomorrow you might be auto-routed to a different Starbucks, but your puzzle is still at the first one! Your puzzle work is "stateful"‚Äîit has state (progress) that's tied to a specific location. For stateful work, you need a regular address (DNS) where you pick and remember which location to go to. Anycast is only for quick, stateless errands where you don't care which location serves you and don't need to go back to the same one. |
| **All Acronyms Explained** | ‚Ä¢ **Anycast** = Same IP announced from multiple locations<br>‚Ä¢ **DNS** = Domain Name System<br>‚Ä¢ **SSH** = Secure Shell (remote terminal)<br>‚Ä¢ **RDP** = Remote Desktop Protocol (Windows remote access)<br>‚Ä¢ **VPN** = Virtual Private Network<br>‚Ä¢ **IPsec** = Internet Protocol Security (VPN encryption)<br>‚Ä¢ **BGP** = Border Gateway Protocol<br>‚Ä¢ **WebSocket** = Protocol for real-time bidirectional communication<br>‚Ä¢ **SLO** = Service Level Objective<br>‚Ä¢ **TCP** = Transmission Control Protocol<br>‚Ä¢ **Session persistence / sticky sessions** = Keeping user on same backend<br>‚Ä¢ **Route 53** = AWS DNS service |
| **Practical Example from Your Portfolio** | **Reference ADR:** `docs/adr/ADR-0004-anycast-vs-dns-routing.md` (created in Q17)<br><br>**Create Runbook:** `docs/runbooks/traffic/affinity-options.md`<br>```markdown<br># Traffic Routing Options for Kuiper Services<br><br>## Service: User Data Plane (Satellite Traffic)<br>- Characteristics: Long-lived (minutes-hours), stateful, requires session persistence<br>- Routing Method: Route 53 Latency + Health Checks<br>- Affinity: Soft (DNS TTL allows graceful migration)<br>- Rationale: Stability over absolute lowest latency<br><br>## Service: Control-Plane API<br>- Characteristics: Short-lived (seconds), stateless (JWT tokens)<br>- Routing Method: ALB + Route 53 Latency<br>- Affinity: None needed<br>- Rationale: Geographic distribution for resilience<br><br>## Service: Telemetry Collection<br>- Characteristics: Short-lived, stateless, high volume<br>- Routing Method: Route 53 Geo + Regional collectors<br>- Affinity: None needed<br>- Rationale: Keep telemetry in-region for data sovereignty<br><br>## Service: DNS Resolver (Hypothetical)<br>- Characteristics: Ultra-short (milliseconds), stateless<br>- Routing Method: Anycast (if we ran own DNS servers)<br>- Affinity: None needed<br>- Rationale: Lowest possible latency, DDoS resistance<br>```<br><br>**Reference:** Your homelab has stateful services (TrueNAS, Proxmox)‚Äîdocument why you use DNS names, not anycast. |
| **Common Pitfalls to Avoid** | ‚ùå Thinking anycast is always better because it's "more advanced"<br>‚ùå Not understanding the stateful vs stateless distinction (critical!)<br>‚ùå Using anycast because big companies do (they have global session stores)<br>‚ùå Forgetting that BGP path changes are unpredictable and frequent<br>‚ùå Assuming application retry logic will save you (users still see errors)<br>‚ùå Not considering the operational complexity of BGP management<br>‚ùå Mixing anycast and DNS for same service (creates routing confusion) |
| **Risk Level** | üî¥ **MEDIUM-HIGH** - Interview trap question; shows you understand trade-offs |
| **Time to Learn** | ‚è±Ô∏è **2-3 hours** - Study examples of anycast failures, understand stateful protocols |
| **Owner/Accountability** | üë§ **You** - This is a "gotcha" question‚Äîknow the answer cold |

---

### Q19: BGP/VPN Monitoring + Synthetic Probes

| Column | Content |
|--------|---------|
| **Question/Topic** | What monitoring is essential for BGP and VPN health? |
| **Core Answer** | **System-Level Metrics (CloudWatch/SNMP):**<br>1. **Tunnel State** (UP/DOWN) - binary health of IPsec tunnel<br>2. **BGP Session State** (ESTABLISHED/IDLE/CONNECT) - is BGP neighbor up?<br>3. **BGP Prefix Count** - how many routes received/advertised (sudden drop = problem)<br>4. **Tunnel Data In/Out (bytes)** - is traffic actually flowing?<br>5. **Tunnel Packet Loss** - % packets dropped<br>6. **BGP Flap Count** - session going up/down repeatedly (very bad)<br><br>**Synthetic Probes (Blackbox Exporter/CloudWatch Synthetics):**<br>7. **ICMP Ping** - end-to-end reachability through tunnel<br>8. **TCP Connect** - can establish connection to service behind tunnel?<br>9. **HTTP/HTTPS** - full application-level check (GET request, validate response)<br>10. **Latency** - P50, P95, P99 round-trip time<br>11. **Jitter** - variation in latency<br><br>**Why Both?** System metrics tell you "is it up?" Synthetics tell you "does it work?" Can have BGP up but traffic black-holed (routing issue). Can have tunnel up but application broken. Need both to catch all failure modes. |
| **Feynman Explanation** | Monitoring BGP/VPN is like monitoring your car's health. System metrics are the dashboard lights (check engine, low oil, flat tire)‚Äîthey tell you when a component is broken. Synthetic probes are actually test-driving the car every minute‚Äîstart it, drive around the block, park it. The lights might say everything is fine, but the test drive reveals the steering pulls left (latency spike) or the brakes are soft (packet loss). You need both: dashboard lights catch obvious failures, test drives catch subtle problems before they become dangerous. |
| **All Acronyms Explained** | ‚Ä¢ **BGP** = Border Gateway Protocol<br>‚Ä¢ **VPN** = Virtual Private Network<br>‚Ä¢ **CloudWatch** = AWS monitoring service<br>‚Ä¢ **SNMP** = Simple Network Management Protocol (network device monitoring)<br>‚Ä¢ **IPsec** = Internet Protocol Security (VPN encryption)<br>‚Ä¢ **ICMP** = Internet Control Message Protocol (ping uses this)<br>‚Ä¢ **TCP** = Transmission Control Protocol<br>‚Ä¢ **HTTP/HTTPS** = Web protocols<br>‚Ä¢ **P50/P95/P99** = 50th/95th/99th percentile (median, 95th, 99th)<br>‚Ä¢ **Jitter** = Latency variation (inconsistent delays)<br>‚Ä¢ **Blackbox Exporter** = Prometheus tool for synthetic monitoring<br>‚Ä¢ **Flap** = Service/session going up and down repeatedly<br>‚Ä¢ **ESTABLISHED** = BGP state meaning session is healthy and exchanging routes |
| **Practical Example from Your Portfolio** | **Reference:** `projects/p04-ops-monitoring/README.md` - Your monitoring project<br><br>**Create Evidence:**<br>1. **Dashboard:** `dashboards/cloudwatch/vpn-bgp-comprehensive.json`<br>   - Panel: Tunnel state over time (UP=1, DOWN=0)<br>   - Panel: BGP session state<br>   - Panel: Bytes in/out per tunnel (stacked area chart)<br>   - Panel: Packet loss % (should be <0.1%)<br>   - Panel: Synthetic probe success rate (should be 100%)<br>   - Panel: Latency heatmap (P50/P95/P99)<br>2. **Synthetic Probes:** `observability/synthetic-probes/`<br>   - `vpn-ping-probe.py` - ICMP through VPN to target behind it<br>   - `vpn-http-probe.py` - HTTP GET to application behind VPN<br>   - Deploy as Lambda function or ECS task<br>3. **Terraform:** `terraform/cloudwatch-vpn-alarms/`<br>   ```hcl<br>   resource "aws_cloudwatch_metric_alarm" "tunnel_down" {<br>     alarm_name          = "vpn-tunnel-1-down"<br>     comparison_operator = "LessThanThreshold"<br>     evaluation_periods  = 2<br>     metric_name         = "TunnelState"<br>     namespace           = "AWS/VPN"<br>     period              = 60<br>     statistic           = "Maximum"<br>     threshold           = 1<br>     alarm_description   = "VPN Tunnel 1 is DOWN"<br>     alarm_actions       = [aws_sns_topic.pagerduty.arn]<br>   }<br>   ```<br>4. **Alert Runbook:** `docs/runbooks/alerts/vpn-tunnel-down.md`<br>   - Symptoms, impact, investigation steps, resolution |
| **Common Pitfalls to Avoid** | ‚ùå Only monitoring tunnel state, not BGP state (tunnel up but BGP down = no routing)<br>‚ùå Only monitoring system metrics, no synthetics (miss application-level issues)<br>‚ùå Alerting on every latency spike (noisy, causes alert fatigue)<br>‚ùå Not monitoring from multiple probe locations (false positives from probe issues)<br>‚ùå Synthetic probes too aggressive (hammering target, causing load)<br>‚ùå Not setting SLO-based alerts (alert on symptoms, not thresholds)<br>‚ùå Missing flap detection (session up/down 10 times = need immediate attention)<br>‚ùå No runbook links in alerts (on-call doesn't know what to do) |
| **Risk Level** | üî¥ **HIGH** - You can't operate what you can't observe |
| **Time to Learn** | ‚è±Ô∏è **5-6 hours** - Build comprehensive monitoring for your VPN project |
| **Owner/Accountability** | üë§ **You** - Showcase this dashboard in your interview |

---

### Q20: Prometheus + Alertmanager Roles

| Column | Content |
|--------|---------|
| **Question/Topic** | Explain the roles of Prometheus vs Alertmanager in monitoring architecture |
| **Core Answer** | **Prometheus:**<br>‚Ä¢ **Scrapes** metrics from exporters every 15-60s (pull model)<br>‚Ä¢ **Stores** time-series data (metrics with timestamps) in local TSDB<br>‚Ä¢ **Evaluates** alerting rules continuously (e.g., "if BGP down for 2 min, fire alert")<br>‚Ä¢ **Sends** fired alerts to Alertmanager<br>‚Ä¢ **Serves** metrics via PromQL queries for dashboards (Grafana)<br>‚Ä¢ Scales vertically (single instance per region/cluster)<br><br>**Alertmanager:**<br>‚Ä¢ **Receives** alerts from one or more Prometheus servers<br>‚Ä¢ **Deduplicates** identical alerts (same alert from multiple Prometheus servers)<br>‚Ä¢ **Groups** alerts by labels (all gateway-a alerts together)<br>‚Ä¢ **Silences** alerts during maintenance windows<br>‚Ä¢ **Inhibits** alerts (if master alert fires, suppress related child alerts)<br>‚Ä¢ **Routes** alerts to correct destination based on labels (gateway=A ‚Üí team-network, severity=critical ‚Üí PagerDuty)<br>‚Ä¢ **Throttles** notification rate (don't page 100 times for same issue)<br>‚Ä¢ Scales horizontally (clustered for HA)<br><br>**Why Both?** Separation of concerns: Prometheus = metric collection & evaluation, Alertmanager = notification intelligence. |
| **Feynman Explanation** | Think of Prometheus as security guards watching video cameras and Alertmanager as the emergency dispatch center. Security guards (Prometheus) watch all the cameras (metrics) and if they see something wrong (rule triggers), they call dispatch (send alert to Alertmanager). Dispatch is smart‚Äîif 5 guards call about the same break-in, dispatch doesn't send 5 police cars, just one (deduplication). If it's a break-in at the bank (label: building=bank), dispatch calls the SWAT team; if it's a break-in at a storage unit (label: building=storage), dispatch calls regular police (routing). During a planned fire drill (maintenance), dispatch ignores all fire alarms (silencing). This division of labor makes the system smarter and prevents chaos! |
| **All Acronyms Explained** | ‚Ä¢ **Prometheus** = Open-source monitoring system & time-series database<br>‚Ä¢ **Alertmanager** = Prometheus's alert routing and management component<br>‚Ä¢ **TSDB** = Time-Series Database (optimized for timestamp + value data)<br>‚Ä¢ **PromQL** = Prometheus Query Language (SQL-like for metrics)<br>‚Ä¢ **Grafana** = Open-source dashboard and visualization platform<br>‚Ä¢ **Exporter** = Service that exposes metrics for Prometheus to scrape (e.g., node_exporter for Linux)<br>‚Ä¢ **HA** = High Availability<br>‚Ä¢ **Scrape** = Pull metrics from target (opposite of push)<br>‚Ä¢ **Labels** = Key-value pairs for identifying metrics (e.g., job="vpn", instance="gw-a", region="us-east-1")<br>‚Ä¢ **Inhibition** = Suppress alerts based on other alerts (alert hierarchy)<br>‚Ä¢ **SWAT** = Special Weapons And Tactics (police team) |
| **Practical Example from Your Portfolio** | **Reference:** `projects/06-homelab/PRJ-HOME-002/` - Your homelab monitoring uses Prometheus<br><br>**Evidence to Create:**<br>1. **Prometheus Rules:** `observability/prometheus/rules/kuiper-gateway.yml`<br>   ```yaml<br>   groups:<br>   - name: kuiper_gateway<br>     interval: 30s<br>     rules:<br>     - alert: VPNTunnelDown<br>       expr: aws_vpn_tunnel_state{tunnel="1"} < 1<br>       for: 2m<br>       labels:<br>         severity: critical<br>         component: vpn<br>         site: gateway-a<br>       annotations:<br>         summary: "VPN Tunnel 1 down at {{ $labels.site }}"<br>         description: "Tunnel has been down for 2+ minutes"<br>         runbook_url: "https://wiki/runbooks/vpn-tunnel-down"<br>     - alert: BGPSessionDown<br>       expr: bgp_session_state != 6  # 6 = ESTABLISHED<br>       for: 1m<br>       labels:<br>         severity: critical<br>         component: bgp<br>         site: "{{ $labels.site }}"<br>   ```<br>2. **Alertmanager Config:** `observability/alertmanager/alertmanager.yml`<br>   ```yaml<br>   route:<br>     receiver: default<br>     group_by: [site, component]<br>     group_wait: 30s<br>     group_interval: 5m<br>     repeat_interval: 4h<br>     routes:<br>     - match:<br>         severity: critical<br>       receiver: pagerduty<br>       continue: true<br>     - match:<br>         component: vpn<br>       receiver: network-team-slack<br>     - match:<br>         component: bgp<br>       receiver: network-team-slack<br>   <br>   inhibit_rules:<br>   - source_match:<br>       alert_name: GatewayDown<br>     target_match_re:<br>       site: "{{ .site }}"<br>     equal: [site]<br>   # If whole gateway is down, don't alert on VPN/BGP at that site<br>   <br>   receivers:<br>   - name: pagerduty<br>     pagerduty_configs:<br>     - service_key: YOUR_PD_KEY<br>   - name: network-team-slack<br>     slack_configs:<br>     - api_url: YOUR_SLACK_WEBHOOK<br>       channel: '#network-ops'<br>   ```<br>3. **Docs:** `docs/observability/alerting-design.md` - Explain your routing logic, inhibition rules, runbook links<br>4. **Reference:** Your homelab monitoring dashboard screenshots |
| **Common Pitfalls to Avoid** | ‚ùå Sending alerts straight from Prometheus (bypassing Alertmanager‚Äîno dedup, no routing)<br>‚ùå Not using labels for routing (all alerts go to same place‚Äînoisy)<br>‚ùå No inhibition rules (alert storm‚Äî100 alerts for one root cause)<br>‚ùå Missing runbook links in alerts (on-call doesn't know what to do)<br>‚ùå No grouping (get paged 50 times instead of 1 grouped page)<br>‚ùå Alerting on metrics not symptoms (disk 80% full vs customer impact)<br>‚ùå Not testing alert routing (wrong alerts go to wrong team)<br>‚ùå Over-silencing (silence too broad, miss real issues during maintenance) |
| **Risk Level** | üü° **MEDIUM-HIGH** - Important for effective monitoring operations |
| **Time to Learn** | ‚è±Ô∏è **4-5 hours** - Study Prometheus & Alertmanager docs, configure for your homelab |
| **Owner/Accountability** | üë§ **You** - Be able to explain your alerting architecture and show your config |

---

## What's Next?

This completes Q1-Q20 covering **Core AWS & Kuiper Ground Infrastructure**.

Continue to the next sections:
- **Q21-Q40:** Observability, Automation, and Homelab (see separate document)
- **Q41-Q60:** SRE Practices, BGP Deep Dive, and Advanced Topics (see separate document)

**Recommended Study Order:**
1. Review all Q1-Q20 tables
2. Create missing artifacts in your portfolio (Terraform, diagrams, runbooks)
3. Practice explaining each topic using Feynman method
4. Run hands-on labs for Q4, Q9, Q10, Q12, Q16, Q19
5. Create ADRs for Q7, Q8, Q11, Q17, Q18
6. Build comprehensive monitoring dashboard (Q19, Q20)
7. Practice interview warm-up questions (see separate document)

---

**Last Updated:** 2025-11-10
**Document:** professional/interview-prep/kuiper/cheat-sheets/02-kuiper-sysadmin-cheatsheet-q11-20.md
