# Project 14: Edge AI Inference Platform

**Category:** Machine Learning & AI
**Status:** ğŸŸ¡ 50% Complete
**Source:** [View Code](https://github.com/samueljackson-collab/Portfolio-Project/tree/main/projects/14-edge-ai)

## Overview

Containerized **ONNX Runtime** microservice optimized for **NVIDIA Jetson** devices with automatic model updates via **Azure IoT Edge**. Enables low-latency AI inference at the edge for computer vision, anomaly detection, and predictive maintenance use cases.

## Key Features

- **Edge Inference** - Sub-100ms latency without cloud dependency
- **Model Optimization** - ONNX format with INT8 quantization for performance
- **OTA Updates** - Azure IoT Edge automatic model deployment
- **Hardware Acceleration** - CUDA/TensorRT optimization for Jetson devices
- **Offline Operation** - Continues functioning during network outages

## Architecture

```
Cloud (Azure)                          Edge (Jetson Device)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Training                         IoT Edge Runtime
     â†“                                       â†“
Model Export (ONNX)                    â”Œâ”€â”€â”€ Edge Module â”€â”€â”€â”
     â†“                                 â†“                    â†“
Azure Container Registry         Inference Service    Local Storage
     â†“                                 â†“                    â†“
IoT Hub â”€â”€â”€â”€â”€â”€ Push Update â”€â”€â”€â”€â”€â”€â†’ Model Update      Results Queue
                                      â†“                    â†“
                             Camera/Sensor Input     Cloud Upload
                                      â†“                (Batched)
                               ONNX Runtime (GPU)
                                      â†“
                               Predictions
```

**Inference Pipeline:**
1. **Model Deployment**: ONNX models pushed via IoT Edge
2. **Input Capture**: Camera/sensor data acquisition
3. **Preprocessing**: Image resizing, normalization
4. **Inference**: ONNX Runtime with TensorRT acceleration
5. **Postprocessing**: Result parsing and formatting
6. **Action**: Local decisions or cloud reporting

## Technologies

- **Python** - Service implementation
- **ONNX Runtime** - Cross-platform inference engine
- **NVIDIA Jetson** - Edge AI hardware (Nano, Xavier, Orin)
- **Azure IoT Edge** - Edge deployment platform
- **TensorRT** - NVIDIA inference optimization
- **Docker** - Containerization
- **OpenCV** - Image processing
- **NumPy** - Numerical operations

## Quick Start

```bash
cd projects/14-edge-ai

# Install dependencies
pip install -r requirements.txt

# Run inference locally (CPU)
python src/inference_service.py \
  --model models/resnet50.onnx \
  --image sample.jpg

# Run with GPU acceleration (Jetson)
python src/inference_service.py \
  --model models/resnet50.onnx \
  --image sample.jpg \
  --device cuda

# Start as service with camera
python src/inference_service.py \
  --model models/resnet50.onnx \
  --camera /dev/video0 \
  --mode stream
```

## Project Structure

```
14-edge-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference_service.py    # Main inference service
â”‚   â”œâ”€â”€ preprocessor.py         # Image preprocessing (to be added)
â”‚   â””â”€â”€ model_manager.py        # OTA model updates (to be added)
â”œâ”€â”€ models/                     # ONNX models (to be added)
â”‚   â”œâ”€â”€ resnet50.onnx
â”‚   â””â”€â”€ yolov8.onnx
â”œâ”€â”€ deployment/                 # IoT Edge manifests (to be added)
â”‚   â””â”€â”€ deployment.template.json
â”œâ”€â”€ Dockerfile                  # Container image (to be added)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Business Impact

- **Latency**: <50ms inference (vs 200ms cloud roundtrip)
- **Bandwidth**: 90% reduction by processing locally
- **Uptime**: 99.9% availability during network outages
- **Cost**: $50/device/month vs $500/month cloud inference
- **Privacy**: Data processed locally, meets GDPR requirements

## Current Status

**Completed:**
- âœ… Core ONNX Runtime inference service
- âœ… Basic image preprocessing
- âœ… CPU and GPU execution paths

**In Progress:**
- ğŸŸ¡ Azure IoT Edge deployment manifest
- ğŸŸ¡ Docker container optimization
- ğŸŸ¡ Model versioning and OTA updates
- ğŸŸ¡ TensorRT optimization

**Next Steps:**
1. Create Dockerfile with multi-stage build
2. Add Azure IoT Edge deployment manifests
3. Implement model update mechanism via IoT Twin
4. Optimize with TensorRT for Jetson devices
5. Add camera integration and video streaming
6. Build model quantization pipeline (INT8)
7. Create monitoring and telemetry reporting
8. Add batch inference for offline processing
9. Document hardware setup and installation

## Key Learning Outcomes

- Edge AI architecture patterns
- ONNX model optimization and deployment
- NVIDIA Jetson development
- Azure IoT Edge platform
- TensorRT acceleration techniques
- Model quantization for edge devices
- Container optimization for embedded systems

---

**Related Projects:**
- [Project 6: MLOps](/projects/06-mlops) - Model training pipeline
- [Project 11: IoT Data Ingestion](/projects/11-iot) - Sensor integration
- [Project 18: GPU Computing](/projects/18-gpu-computing) - GPU optimization patterns
