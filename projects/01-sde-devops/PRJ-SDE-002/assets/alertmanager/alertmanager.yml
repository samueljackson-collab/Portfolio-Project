# Alertmanager configuration for homelab observability stack
# ----------------------------------------------------------
# This file defines notification routes, receiver integrations, and inhibition
# logic tailored for a small team operating a Proxmox-based homelab.

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'prometheus@homelab.local'
  smtp_require_tls: true
  slack_api_url: 'https://hooks.slack.com/services/PLACEHOLDER/WEBHOOK/URL'

route:
  receiver: default-email
  group_by: ['cluster', 'alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  routes:
    - receiver: critical-all
      matchers:
        - severity="critical"
        - component="infrastructure"
      group_wait: 10s
      repeat_interval: 30m
    - receiver: backup-team
      matchers:
        - severity="critical"
        - component="backup"
      group_by: ['job_name']
      repeat_interval: 12h
    - receiver: slack-only
      matchers:
        - severity="warning"
      repeat_interval: 24h
    - receiver: app-team
      matchers:
        - component="application"
      group_by: ['service_name', 'alertname']

receivers:
  - name: default-email
    email_configs:
      - to: 'alerts@homelab.local'
        headers:
          subject: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
        html: |
          <html>
            <body>
              <h3>Alert summary ({{ .Status }})</h3>
              <table border="1" cellpadding="4" cellspacing="0">
                <tr><th>Alert</th><th>Instance</th><th>Severity</th><th>Description</th></tr>
                {{ range .Alerts }}
                <tr>
                  <td>{{ .Labels.alertname }}</td>
                  <td>{{ .Labels.instance }}</td>
                  <td>{{ .Labels.severity }}</td>
                  <td>{{ .Annotations.description }}</td>
                </tr>
                {{ end }}
              </table>
              <p>Runbook: {{ (index .Alerts 0).Annotations.runbook }}</p>
            </body>
          </html>

  - name: critical-all
    email_configs:
      - to: 'alerts@homelab.local'
        headers:
          subject: '[CRITICAL] {{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
    slack_configs:
      - channel: '#homelab-critical'
        username: 'Prometheus Alertmanager'
        title: 'Critical Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}• {{ .Labels.instance }} - {{ .Annotations.description }}\n{{ end }}'
        color: 'danger'
    pagerduty_configs:
      - service_key: 'pg123456'
        severity: 'critical'
        description: '{{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'

  - name: backup-team
    email_configs:
      - to: 'alerts@homelab.local'
        headers:
          subject: '[BACKUP] {{ .GroupLabels.alertname }} for {{ .GroupLabels.job_name }}'

  - name: slack-only
    slack_configs:
      - channel: '#homelab-alerts'
        username: 'Prometheus Alertmanager'
        title: 'Warning Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}• {{ .Labels.instance }} - {{ .Annotations.description }}\n{{ end }}'
        color: 'warning'

  - name: app-team
    email_configs:
      - to: 'alerts@homelab.local'
        headers:
          subject: '[APP] {{ .GroupLabels.alertname }} affecting {{ .GroupLabels.service_name }}'
    slack_configs:
      - channel: '#homelab-alerts'
        username: 'Prometheus Alertmanager'
        title: 'Application Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}• {{ .Labels.service_name }} ({{ .Labels.instance }}) - {{ .Annotations.description }}\n{{ end }}'
        color: '#ff9800'

inhibit_rules:
  - source_matchers:
      - alertname="HostDown"
    target_matchers:
      - alertname=~".*"
    equal: ['instance']
  - source_matchers:
      - severity="critical"
    target_matchers:
      - severity="warning"
    equal: ['alertname', 'instance']

# Example silences (uncomment and adjust when needed):
# silences:
#   - matcher: 'instance="192.168.1.20:9100"'
#     comment: 'Maintenance window for VM1'
#     createdBy: 'admin'
#     startsAt: '2024-01-01T02:00:00Z'
#     endsAt: '2024-01-01T04:00:00Z'

# Testing receivers:
# - amtool alert add HostDown severity=critical instance=192.168.1.20:9100 --expires=5m
# - amtool config routes
# - amtool silence add alertname=DiskSpaceLow instance=192.168.1.22:9100 --duration=2h
