###############################################################################
# Enterprise Monitoring & Observability Stack
# Docker Compose Configuration
###############################################################################
#
# Purpose: Production-ready monitoring solution for infrastructure observability
# Stack: Prometheus, Grafana, Loki, Promtail, Alertmanager, Node Exporter, cAdvisor
#
# Architecture:
#   - Frontend Network: External access to Grafana UI
#   - Backend Network: Internal communication between monitoring components
#   - Isolated container metrics collection
#   - Centralized log aggregation
#   - Multi-channel alerting
#
# Resource Requirements:
#   - Memory: ~6GB total (Prometheus 2GB, Grafana 1GB, others 500MB-1GB each)
#   - CPU: ~3 cores total
#   - Disk: 50GB for 15-day metric retention + 7-day log retention
#
# Security:
#   - All services bind to localhost (127.0.0.1) or internal networks
#   - No external exposure without reverse proxy
#   - Secrets managed via .env file (not committed to git)
#
# Usage:
#   docker-compose up -d              # Start all services
#   docker-compose ps                 # Check service status
#   docker-compose logs -f [service]  # View logs
#   docker-compose down               # Stop all services
#
###############################################################################

version: '3.8'

###############################################################################
# NETWORKS
###############################################################################
# Two-network architecture for security isolation:
# - monitoring_frontend: Grafana UI access (can be exposed via reverse proxy)
# - monitoring_backend: Internal metrics/logs collection (isolated)
networks:
  # Frontend network for user-facing services
  # Only Grafana connects to this network for UI access
  monitoring_frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/24
    labels:
      - "com.docker.compose.network=frontend"
      - "purpose=user-interface"

  # Backend network for internal monitoring communication
  # All monitoring components communicate here
  # Isolated from external access
  monitoring_backend:
    driver: bridge
    internal: false  # Set to true in production to prevent internet access
    ipam:
      config:
        - subnet: 172.29.0.0/24
    labels:
      - "com.docker.compose.network=backend"
      - "purpose=metrics-collection"

###############################################################################
# VOLUMES
###############################################################################
# Named volumes for persistent data storage
# Survives container recreation, enables backup/restore
volumes:
  # Prometheus time-series database storage
  # Contains all metrics data, indexes, and write-ahead log (WAL)
  # Size estimation: 15 days * 50K samples/sec * 1.5 bytes ≈ 97GB
  prometheus_data:
    driver: local
    labels:
      - "backup.policy=daily"
      - "retention=15d"

  # Grafana configuration and dashboard storage
  # Contains datasources, dashboards, users, preferences
  # Typically <1GB even with many dashboards
  grafana_data:
    driver: local
    labels:
      - "backup.policy=daily"
      - "retention=indefinite"

  # Loki log storage
  # Contains log chunks and indexes
  # Size estimation: 7 days * log volume (varies widely)
  loki_data:
    driver: local
    labels:
      - "backup.policy=weekly"
      - "retention=7d"

  # Alertmanager configuration and state
  # Contains alert state, silences, and notification history
  # Typically <100MB
  alertmanager_data:
    driver: local
    labels:
      - "backup.policy=daily"
      - "retention=30d"

###############################################################################
# SERVICES
###############################################################################

services:
  ###########################################################################
  # PROMETHEUS - Metrics Collection & Storage
  ###########################################################################
  # Purpose: Time-series metrics database and scraping engine
  # Collects metrics from all exporters and services
  # Evaluates alert rules every 15 seconds
  # Provides query API for Grafana visualization
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: monitoring-prometheus
    user: "nobody"  # Run as non-root for security

    # Command-line arguments configure Prometheus behavior
    command:
      # --config.file: Path to main Prometheus configuration
      # Mounted as volume to enable hot-reload without container restart
      - '--config.file=/etc/prometheus/prometheus.yml'

      # --storage.tsdb.path: Time-series database storage location
      # Uses named volume 'prometheus_data' for persistence across restarts
      - '--storage.tsdb.path=/prometheus'

      # --storage.tsdb.retention.time: Data retention period
      # Set to 15 days balancing observability needs vs disk space
      # Calculate: 15 days * 50K samples/s * 1.5 bytes/sample ≈ 97GB
      # Older data automatically deleted to maintain retention
      - '--storage.tsdb.retention.time=15d'

      # --web.enable-lifecycle: Enable API endpoints for hot-reload
      # Allows 'curl -X POST localhost:9090/-/reload' to reload config
      # without restarting container (zero downtime config updates)
      - '--web.enable-lifecycle'

      # --web.console.libraries: Path to console template libraries
      # Enables Prometheus web UI custom console templates
      - '--web.console.libraries=/etc/prometheus/console_libraries'

      # --web.console.templates: Path to console templates
      # Provides pre-built dashboards in Prometheus UI
      - '--web.console.templates=/etc/prometheus/consoles'

      # --storage.tsdb.wal-compression: Enable WAL compression
      # Reduces disk I/O and storage for write-ahead log
      # Minimal CPU overhead, significant space savings (50-70%)
      - '--storage.tsdb.wal-compression'

    # Volume Mounts: Configuration and persistent data
    volumes:
      # Configuration file mount (read-only for safety)
      # Source: ./prometheus/prometheus.yml in project directory
      # Target: /etc/prometheus/prometheus.yml in container
      # Mode: :ro prevents accidental modification by container processes
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro

      # Alert rules mount (read-only)
      # Contains all alerting rules evaluated every 15s
      # Separate file for easier management and hot-reload capability
      - ./prometheus/alerts:/etc/prometheus/alerts:ro

      # Console templates (optional, for Prometheus UI)
      - ./prometheus/consoles:/etc/prometheus/consoles:ro
      - ./prometheus/console_libraries:/etc/prometheus/console_libraries:ro

      # Persistent data volume (read-write)
      # Named volume managed by Docker, survives container recreation
      # Contains all time-series data, indexes, and WAL
      - prometheus_data:/prometheus

    # Network Configuration
    # Rationale: Dual network approach for security
    networks:
      # frontend: Allows Grafana to query Prometheus
      # backend: Allows Prometheus to scrape exporters
      - monitoring_frontend
      - monitoring_backend

    # Port Binding
    # Binds to localhost only for security (not exposed to network)
    # Access via reverse proxy or SSH tunnel in production
    # Format: "host:container" where host is 127.0.0.1 (localhost only)
    ports:
      - "127.0.0.1:9090:9090"  # Web UI and API endpoint

    # Health Check Configuration
    # Validates Prometheus is responsive and database is healthy
    # Prevents declaring service "up" before fully initialized
    healthcheck:
      # Test command: HTTP GET to /-/healthy endpoint
      # --no-verbose: Suppress wget output for cleaner logs
      # --tries=1: Single attempt, fail fast
      # --spider: Don't download, just check if resource exists
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]

      # interval: Time between health checks (30s typical)
      # Balances responsiveness vs. resource usage
      interval: 30s

      # timeout: Max time to wait for health check response
      # 10s allows for temporarily slow responses without false negative
      timeout: 10s

      # retries: Number of consecutive failures before marking unhealthy
      # 3 retries prevents transient issues from triggering restart
      retries: 3

      # start_period: Grace period during container initialization
      # 40s allows Prometheus to load TSDB, rules, and start scraping
      # Health checks during this period don't count toward retries
      start_period: 40s

    # Restart Policy
    # unless-stopped: Always restart except when explicitly stopped
    # Ensures monitoring stays up despite host reboots or crashes
    restart: unless-stopped

    # Resource Limits
    # Prevents resource exhaustion on shared host
    deploy:
      resources:
        # Limits: Maximum resources container can use
        # Enforced by cgroups, container killed if exceeded
        limits:
          # 2GB memory: Handles ~100K samples/sec with 15d retention
          # Calculation based on: (samples/s * retention * compression)
          memory: 2G

          # 1.0 CPU: One full core for query execution and scraping
          # Prometheus is CPU-bound during query evaluation
          cpus: "1.0"

        # Reservations: Guaranteed minimum resources
        # Scheduler ensures these resources available before placement
        reservations:
          # 1GB reserved: Ensures baseline performance
          # Prevents OOM during startup when loading TSDB
          memory: 1G

          # 0.5 CPU reserved: Guarantees scraping continues
          # Even under host load, scraping won't be starved
          cpus: "0.5"

    # Dependency Management
    # Ensures required services start before Prometheus
    depends_on:
      alertmanager:
        # condition: service_healthy waits for health check pass
        # Reason: Prometheus validates alertmanager endpoint on startup
        condition: service_healthy

    # Labels: Metadata for container management and filtering
    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=metrics"
      - "backup.policy=daily"

  ###########################################################################
  # GRAFANA - Visualization & Dashboards
  ###########################################################################
  # Purpose: Metrics visualization and dashboard platform
  # Queries Prometheus and Loki for data
  # Provides web UI for exploring metrics and logs
  # Supports alerting (though we use Alertmanager primarily)
  grafana:
    image: grafana/grafana:10.2.3
    container_name: monitoring-grafana
    user: "472"  # Grafana user (non-root)

    # Environment variables for Grafana configuration
    # Prefer env vars over grafana.ini for container deployments
    environment:
      # Security Settings
      # Admin credentials (change in .env file!)
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-admin}

      # Server Configuration
      # root_url should match your reverse proxy setup
      - GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-http://localhost:3000}
      - GF_SERVER_SERVE_FROM_SUB_PATH=false

      # Plugin Installation
      # Comma-separated list of plugins to auto-install
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel

      # Anonymous Access (disabled for security)
      - GF_AUTH_ANONYMOUS_ENABLED=false

      # Disable Grafana Update Checking
      # Prevents outbound connections for update checks
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false

      # Logging Configuration
      - GF_LOG_LEVEL=info
      - GF_LOG_MODE=console

    # Volume Mounts
    volumes:
      # Grafana persistent data (dashboards, users, datasources)
      # This volume is critical - contains all dashboard configurations
      - grafana_data:/var/lib/grafana

      # Provisioning configurations (auto-setup datasources & dashboards)
      # Read-only: These are declarative configs loaded at startup
      # Allows GitOps-style dashboard management
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro

      # Dashboard JSON files
      # Pre-built dashboards loaded automatically on startup
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro

    # Network Configuration
    networks:
      # frontend: Serves web UI to users
      # backend: Queries Prometheus/Loki
      - monitoring_frontend
      - monitoring_backend

    # Port Binding
    # Bind to localhost for security
    # Use reverse proxy (nginx/traefik) for external access
    ports:
      - "127.0.0.1:3000:3000"  # Web UI

    # Health Check
    # Grafana provides /api/health endpoint
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    restart: unless-stopped

    # Resource Limits
    # Grafana is less resource-intensive than Prometheus
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 512M
          cpus: "0.25"

    # Dependencies
    # Wait for data sources to be available before starting
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=visualization"
      - "backup.policy=daily"

  ###########################################################################
  # LOKI - Log Aggregation
  ###########################################################################
  # Purpose: Horizontally scalable log aggregation system
  # Stores logs as compressed chunks indexed by labels
  # Provides LogQL query language for log exploration
  # Integrates with Grafana for unified metrics + logs view
  loki:
    image: grafana/loki:2.9.3
    container_name: monitoring-loki
    user: "10001"  # Loki user (non-root)

    # Command specifies config file location
    command: -config.file=/etc/loki/loki-config.yml

    volumes:
      # Loki configuration file
      # Contains storage, retention, and ingestion configs
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro

      # Persistent storage for log chunks and indexes
      # Named volume survives container recreation
      - loki_data:/loki

    networks:
      - monitoring_backend

    # Port Binding
    # 3100: HTTP API for log ingestion and queries
    ports:
      - "127.0.0.1:3100:3100"

    # Health Check
    # Loki provides /ready endpoint
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    restart: unless-stopped

    # Resource Limits
    # Loki can be memory-intensive with high log volumes
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 512M
          cpus: "0.25"

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=logs"
      - "backup.policy=weekly"

  ###########################################################################
  # PROMTAIL - Log Shipping Agent
  ###########################################################################
  # Purpose: Collects logs and ships to Loki
  # Scrapes log files, Docker container logs, journald
  # Applies labels and parsing before sending to Loki
  # Lightweight agent, one per host
  promtail:
    image: grafana/promtail:2.9.3
    container_name: monitoring-promtail
    user: "root"  # Needs root to access Docker socket and system logs

    command: -config.file=/etc/promtail/promtail-config.yml

    volumes:
      # Promtail configuration
      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro

      # Docker socket for container log collection
      # Read-only mount for security
      # Allows Promtail to read container logs
      - /var/run/docker.sock:/var/run/docker.sock:ro

      # Docker container logs directory
      # Contains JSON logs from all containers
      - /var/lib/docker/containers:/var/lib/docker/containers:ro

      # System logs
      # Allows collection of syslog, auth.log, etc.
      - /var/log:/var/log:ro

    networks:
      - monitoring_backend

    # Port Binding
    # 9080: Promtail metrics endpoint (scraped by Prometheus)
    ports:
      - "127.0.0.1:9080:9080"

    # Health Check
    # Promtail provides /ready endpoint
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

    restart: unless-stopped

    # Resource Limits
    # Promtail is lightweight but can spike with log bursts
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.25"
        reservations:
          memory: 128M
          cpus: "0.1"

    depends_on:
      loki:
        condition: service_healthy

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=log-shipper"

  ###########################################################################
  # ALERTMANAGER - Alert Routing & Management
  ###########################################################################
  # Purpose: Handles alerts from Prometheus
  # Routes alerts to appropriate channels (email, Slack, PagerDuty)
  # Manages alert deduplication, grouping, and silencing
  # Provides web UI for alert management
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: monitoring-alertmanager
    user: "nobody"

    command:
      # --config.file: Alert routing configuration
      - '--config.file=/etc/alertmanager/alertmanager.yml'

      # --storage.path: Persistent storage for alert state
      # Maintains state across restarts (silences, notification history)
      - '--storage.path=/alertmanager'

      # --web.external-url: URL for alert links in notifications
      # Set this to your reverse proxy URL in production
      - '--web.external-url=http://localhost:9093'

    volumes:
      # Alertmanager configuration
      # Contains routing rules and receiver definitions
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro

      # Persistent storage for alert state
      - alertmanager_data:/alertmanager

    networks:
      - monitoring_frontend
      - monitoring_backend

    # Port Binding
    # 9093: Web UI and API
    ports:
      - "127.0.0.1:9093:9093"

    # Health Check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

    restart: unless-stopped

    # Resource Limits
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.25"
        reservations:
          memory: 128M
          cpus: "0.1"

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=alerting"
      - "backup.policy=daily"

  ###########################################################################
  # NODE EXPORTER - Host Metrics
  ###########################################################################
  # Purpose: Exports hardware and OS metrics from host
  # Provides CPU, memory, disk, network metrics
  # Standard exporter for Linux systems
  # One instance per host (this runs on Docker host)
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: monitoring-node-exporter
    user: "root"  # Needs root to access system metrics

    # Command enables specific collectors
    # Disable unnecessary collectors to reduce overhead
    command:
      - '--path.rootfs=/host'
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

    volumes:
      # Mount host filesystem as read-only
      # Allows node-exporter to read system metrics
      - /:/host:ro,rslave
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro

    networks:
      - monitoring_backend

    # Port Binding
    # 9100: Metrics endpoint (scraped by Prometheus)
    ports:
      - "127.0.0.1:9100:9100"

    # Health Check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

    restart: unless-stopped

    # Resource Limits
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.15"
        reservations:
          memory: 64M
          cpus: "0.05"

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=exporter"
      - "monitoring.type=node"

  ###########################################################################
  # CADVISOR - Container Metrics
  ###########################################################################
  # Purpose: Exports container resource usage and performance metrics
  # Provides per-container CPU, memory, disk, network metrics
  # Essential for monitoring containerized workloads
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    container_name: monitoring-cadvisor
    user: "root"  # Needs root for Docker socket access

    # cAdvisor has no config file, runs with sensible defaults
    # Automatically discovers all containers on host

    volumes:
      # Docker socket for container discovery and metrics
      - /var/run/docker.sock:/var/run/docker.sock:ro

      # Host filesystem for container cgroup metrics
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro

    networks:
      - monitoring_backend

    # Port Binding
    # 8080: Web UI and metrics endpoint
    ports:
      - "127.0.0.1:8080:8080"

    # Health Check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

    restart: unless-stopped

    # Resource Limits
    # cAdvisor can be resource-intensive with many containers
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
        reservations:
          memory: 256M
          cpus: "0.25"

    # Security Options
    # Privileged mode required for full container metrics access
    privileged: true
    devices:
      - /dev/kmsg

    labels:
      - "com.docker.compose.project=monitoring-stack"
      - "monitoring.role=exporter"
      - "monitoring.type=container"

###############################################################################
# END OF DOCKER COMPOSE CONFIGURATION
###############################################################################
