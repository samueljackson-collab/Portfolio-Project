# Alertmanager routing tree with clear documentation for escalation paths and noise reduction.
route:
  receiver: default
  group_by: [cluster, alertname, severity]
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  routes:
    - matchers:
        - severity="critical"
      receiver: pagerduty-critical
      group_wait: 0s
      group_interval: 5m
      repeat_interval: 4h
      # Critical alerts page immediately; shorter repeat to keep on-call informed.
    - matchers:
        - severity="warning"
      receiver: slack-notifications
      group_wait: 5m
      group_interval: 10m
      repeat_interval: 12h
      # Warning alerts batch for 5m to reduce noise from transient spikes.

defaults: {}

receivers:
  - name: default
    slack_configs:
      - api_url: ${ALERTMANAGER_SLACK_WEBHOOK_URL}
        channel: "#monitoring"
        send_resolved: true
    email_configs:
      - to: ${ALERTMANAGER_SMTP_TO}
        from: ${ALERTMANAGER_SMTP_FROM}
        smarthost: ${ALERTMANAGER_SMTP_HOST}
        auth_username: ${ALERTMANAGER_SMTP_USERNAME}
        auth_password: ${ALERTMANAGER_SMTP_PASSWORD}
        require_tls: true

  - name: slack-notifications
    slack_configs:
      - api_url: ${ALERTMANAGER_SLACK_WEBHOOK_URL}
        channel: "#monitoring-warning"
        send_resolved: true
        title: "{{ .CommonLabels.alertname }} ({{ .CommonLabels.severity }})"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"

  - name: email-alerts
    email_configs:
      - to: ${ALERTMANAGER_SMTP_TO}
        from: ${ALERTMANAGER_SMTP_FROM}
        smarthost: ${ALERTMANAGER_SMTP_HOST}
        auth_username: ${ALERTMANAGER_SMTP_USERNAME}
        auth_password: ${ALERTMANAGER_SMTP_PASSWORD}
        require_tls: true
        headers:
          subject: "[Monitoring] {{ .CommonLabels.alertname }}"

  - name: pagerduty-critical
    pagerduty_configs:
      - routing_key: ${PAGERDUTY_ROUTING_KEY}
        severity: critical

inhibit_rules:
  - source_matchers: ["severity='critical'"]
    target_matchers: ["severity='warning'"]
    equal: ["alertname", "instance"]
    # Suppress warning duplicates when a critical alert already active for same instance.
  - source_matchers: ["alertname='InstanceDown'"]
    target_matchers: ["alertname=~'DiskSpaceLow.*'"]
    equal: ["instance"]
    # Avoid storm of disk alerts when host is already unreachable.

# Testing examples:
# - Fire a test alert: `amtool alert add TestAlert instance=localhost severity=warning`
# - Check routing: `amtool config routes`
