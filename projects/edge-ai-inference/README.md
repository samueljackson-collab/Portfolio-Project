# Edge AI Inference Platform

**Category:** AI/ML          **Status:** Complete

## Overview
ONNX Runtime service optimized for edge accelerators.

## Tech Stack
ONNX, TensorRT, Python

## Running Locally
1. Clone the repo and navigate to `edge-ai-inference`.
2. Follow the service-specific README or docker-compose file to start dependencies.
3. Run the test suite to validate changes.

## Links
- Portfolio API: http://localhost:8000
- Frontend: http://localhost:5173
