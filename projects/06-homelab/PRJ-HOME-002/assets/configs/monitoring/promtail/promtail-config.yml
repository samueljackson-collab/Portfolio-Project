# Promtail Configuration - Complete Production Setup
# =====================================================
# Version: Promtail 2.9.0
# Purpose: Log shipping agent for Loki log aggregation
# Last Updated: November 6, 2025
#
# Configuration Philosophy:
# ------------------------
# This Promtail configuration is designed to collect logs from multiple sources:
# - System logs (/var/log/syslog, /var/log/auth.log, etc.)
# - Docker container logs (JSON format)
# - Nginx access and error logs
# - PostgreSQL logs
# - Application logs (JSON structured logs)
# - Custom service logs
#
# Key Design Decisions:
# ---------------------
# 1. Multiple Scrape Configs:
#    - Each log source has dedicated scrape config
#    - Enables source-specific parsing and labeling
#    - Allows independent rate limiting per source
#
# 2. Pipeline Stages:
#    - Regex extraction for unstructured logs
#    - JSON parsing for structured logs
#    - Timestamp parsing for accurate ordering
#    - Label extraction for efficient querying
#    - Label drops to reduce cardinality
#
# 3. Label Strategy:
#    - Low-cardinality labels (job, host, service)
#    - High-cardinality data in log lines, not labels
#    - Static labels for environment/cluster
#    - Dynamic labels from log content (limited)
#
# 4. Batching:
#    - 1-second batch wait (balance latency vs. efficiency)
#    - 1MB batch size (optimize network usage)
#    - Automatic retries with backoff
#
# 5. Position Tracking:
#    - Save read position to /tmp/positions.yaml
#    - Prevents duplicate log ingestion on restart
#    - Essential for reliable log collection
#
# =====================================================

# =====================================================
# SERVER CONFIGURATION
# =====================================================
server:
  http_listen_port: 9080
  grpc_listen_port: 0  # Disable gRPC server (not needed)
  log_level: info
  log_format: logfmt

  # Health check endpoints
  # http://localhost:9080/ready
  # http://localhost:9080/metrics

# =====================================================
# POSITIONS (Read State Tracking)
# =====================================================
positions:
  filename: /tmp/positions.yaml
  sync_period: 10s
  ignore_invalid_yaml: false

# =====================================================
# LOKI CLIENT CONFIGURATION
# =====================================================
clients:
  - url: http://192.168.40.30:3100/loki/api/v1/push

    # Batching configuration
    batchwait: 1s        # Wait max 1 second before sending
    batchsize: 1048576   # Send when batch reaches 1MB

    # Retry configuration
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10

    # Timeouts
    timeout: 10s

    # External labels (applied to all logs)
    external_labels:
      environment: homelab
      cluster: primary

# =====================================================
# SCRAPE CONFIGURATIONS
# =====================================================
scrape_configs:

  # ===================================================
  # SCRAPE CONFIG 1: System Syslog
  # ===================================================
  # Collects general system logs from /var/log/syslog
  # Parses standard syslog format: timestamp hostname process[pid]: message
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: syslog
          host: ${HOSTNAME}
          __path__: /var/log/syslog

    # Pipeline to parse syslog format
    pipeline_stages:
      # Extract timestamp, hostname, process, PID, and message
      - regex:
          expression: '^(?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+(?P<hostname>\S+)\s+(?P<process>\w+)(\[(?P<pid>\d+)\])?:\s+(?P<message>.*)$'

      # Parse timestamp into native format
      - timestamp:
          source: timestamp
          format: 'Jan 2 15:04:05'
          action_on_failure: fudge

      # Extract labels from parsed fields
      - labels:
          hostname:
          process:

      # Output only the message (metadata in labels)
      - output:
          source: message

  # ===================================================
  # SCRAPE CONFIG 2: Authentication Logs
  # ===================================================
  # Collects security-critical auth logs
  # Used for monitoring SSH attempts, sudo usage, login failures
  - job_name: auth
    static_configs:
      - targets:
          - localhost
        labels:
          job: auth
          host: ${HOSTNAME}
          tier: security
          __path__: /var/log/auth.log

    pipeline_stages:
      # Parse syslog format
      - regex:
          expression: '^(?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+(?P<hostname>\S+)\s+(?P<process>\w+)(\[(?P<pid>\d+)\])?:\s+(?P<message>.*)$'

      - timestamp:
          source: timestamp
          format: 'Jan 2 15:04:05'

      # Extract authentication events
      - regex:
          source: message
          expression: '(?P<auth_event>Failed password|Accepted publickey|sudo|COMMAND)'
          action_on_failure: pass

      - labels:
          process:
          auth_event:

      - output:
          source: message

  # ===================================================
  # SCRAPE CONFIG 3: Docker Container Logs
  # ===================================================
  # Collects logs from all Docker containers
  # Docker logs are JSON formatted by default
  - job_name: docker
    static_configs:
      - targets:
          - localhost
        labels:
          job: docker
          host: ${HOSTNAME}
          __path__: /var/lib/docker/containers/*/*.log

    pipeline_stages:
      # Parse Docker's JSON log format
      - json:
          expressions:
            log: log
            stream: stream
            time: time
            attrs: attrs

      # Extract container metadata from file path
      - regex:
          source: filename
          expression: '/var/lib/docker/containers/(?P<container_id>[^/]+)/.*\.log'

      # Parse timestamp
      - timestamp:
          source: time
          format: RFC3339Nano

      # Label by stream (stdout/stderr)
      - labels:
          stream:
          container_id:

      # Output the actual log line
      - output:
          source: log

      # Drop empty log lines
      - match:
          selector: '{job="docker"}'
          stages:
            - regex:
                expression: '^\s*$'
                action: drop

  # ===================================================
  # SCRAPE CONFIG 4: Nginx Access Logs
  # ===================================================
  # Collects HTTP access logs from Nginx reverse proxy
  # Parses combined log format for rich metadata
  - job_name: nginx_access
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx_access
          host: ${HOSTNAME}
          service: nginx
          tier: infrastructure
          __path__: /var/log/nginx/access.log

    pipeline_stages:
      # Parse combined log format
      # Example: 192.168.1.100 - - [06/Nov/2025:10:30:45 +0000] "GET /api/users HTTP/1.1" 200 1234 "-" "Mozilla/5.0"
      - regex:
          expression: '^(?P<remote_addr>\S+)\s+\S+\s+(?P<remote_user>\S+)\s+\[(?P<time_local>[^\]]+)\]\s+"(?P<method>\S+)\s+(?P<path>\S+)\s+(?P<protocol>\S+)"\s+(?P<status>\d+)\s+(?P<body_bytes_sent>\d+)\s+"(?P<http_referer>[^"]*)"\s+"(?P<http_user_agent>[^"]*)"'

      # Parse timestamp
      - timestamp:
          source: time_local
          format: '02/Jan/2006:15:04:05 -0700'

      # Extract labels (keep low cardinality)
      - labels:
          method:
          status:

      # Drop health check requests (reduce noise)
      - match:
          selector: '{job="nginx_access"} |~ "GET /health"'
          action: drop

      # Drop static asset requests (optional, reduces volume)
      - match:
          selector: '{job="nginx_access"} |~ "\\.(css|js|png|jpg|ico)$"'
          action: drop

  # ===================================================
  # SCRAPE CONFIG 5: Nginx Error Logs
  # ===================================================
  # Collects Nginx error logs (critical for troubleshooting)
  - job_name: nginx_error
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx_error
          host: ${HOSTNAME}
          service: nginx
          tier: infrastructure
          severity: error
          __path__: /var/log/nginx/error.log

    pipeline_stages:
      # Parse Nginx error format
      # Example: 2025/11/06 10:30:45 [error] 1234#1234: *5678 connect() failed (111: Connection refused)
      - regex:
          expression: '^(?P<timestamp>\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2})\s+\[(?P<level>\w+)\]\s+(?P<pid>\d+)#(?P<tid>\d+):\s+(\*(?P<connection_id>\d+)\s+)?(?P<message>.*)$'

      - timestamp:
          source: timestamp
          format: '2006/01/02 15:04:05'

      - labels:
          level:

  # ===================================================
  # SCRAPE CONFIG 6: PostgreSQL Logs
  # ===================================================
  # Collects PostgreSQL database logs
  # Parses log_line_prefix format: %t [%p] %u@%d
  - job_name: postgresql
    static_configs:
      - targets:
          - localhost
        labels:
          job: postgresql
          host: ${HOSTNAME}
          service: postgresql
          tier: database
          __path__: /var/log/postgresql/postgresql-*.log

    pipeline_stages:
      # Parse PostgreSQL log format
      # Example: 2025-11-06 10:30:45 UTC [1234] user@database LOG: statement: SELECT * FROM users
      - regex:
          expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\s+\w+)\s+\[(?P<pid>\d+)\]\s+(?P<user>\w+)@(?P<database>\w+)\s+(?P<level>\w+):\s+(?P<message>.*)$'

      - timestamp:
          source: timestamp
          format: '2006-01-02 15:04:05 MST'

      # Extract labels (avoid user/database for cardinality)
      - labels:
          level:

      # Drop connection logs (reduce noise)
      - match:
          selector: '{job="postgresql"} |~ "(connection received|connection authorized)"'
          action: drop

  # ===================================================
  # SCRAPE CONFIG 7: JSON Application Logs (Immich)
  # ===================================================
  # Collects structured JSON logs from Immich application
  - job_name: immich
    static_configs:
      - targets:
          - localhost
        labels:
          job: immich
          host: ${HOSTNAME}
          service: immich
          tier: application
          __path__: /var/log/immich/*.log

    pipeline_stages:
      # Parse JSON log format
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            context: context
            trace_id: trace_id
            user_id: user_id
            request_id: request_id

      # Parse ISO8601 timestamp
      - timestamp:
          source: timestamp
          format: RFC3339

      # Extract labels
      - labels:
          level:
          context:

      # Drop trace_id and request_id from labels (too high cardinality)
      # Keep them in the log message instead
      - labeldrop:
          - trace_id
          - request_id
          - user_id

  # ===================================================
  # SCRAPE CONFIG 8: JSON Application Logs (Home Assistant)
  # ===================================================
  # Collects structured logs from Home Assistant
  - job_name: homeassistant
    static_configs:
      - targets:
          - localhost
        labels:
          job: homeassistant
          host: ${HOSTNAME}
          service: home-assistant
          tier: application
          __path__: /var/log/homeassistant/home-assistant.log

    pipeline_stages:
      # Home Assistant logs can be JSON or plain text
      # Try JSON first, fallback to plain text
      - match:
          selector: '{job="homeassistant"}'
          stages:
            - json:
                expressions:
                  timestamp: timestamp
                  level: level
                  message: message
                  logger: logger
                source: content

            - timestamp:
                source: timestamp
                format: RFC3339
                action_on_failure: fudge

            - labels:
                level:
                logger:

  # ===================================================
  # SCRAPE CONFIG 9: JSON Application Logs (Wiki.js)
  # ===================================================
  # Collects logs from Wiki.js application
  - job_name: wikijs
    static_configs:
      - targets:
          - localhost
        labels:
          job: wikijs
          host: ${HOSTNAME}
          service: wiki.js
          tier: application
          __path__: /var/log/wikijs/*.log

    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            module: module

      - timestamp:
          source: timestamp
          format: RFC3339

      - labels:
          level:
          module:

  # ===================================================
  # SCRAPE CONFIG 10: Kernel Logs (dmesg)
  # ===================================================
  # Collects kernel messages (hardware issues, driver problems)
  - job_name: kernel
    static_configs:
      - targets:
          - localhost
        labels:
          job: kernel
          host: ${HOSTNAME}
          tier: system
          __path__: /var/log/kern.log

    pipeline_stages:
      # Parse syslog format
      - regex:
          expression: '^(?P<timestamp>\w+\s+\d+\s+\d+:\d+:\d+)\s+(?P<hostname>\S+)\s+kernel:\s+\[[\s\d.]+\]\s+(?P<message>.*)$'

      - timestamp:
          source: timestamp
          format: 'Jan 2 15:04:05'

      # Extract subsystem from message (e.g., "usb", "disk", "net")
      - regex:
          source: message
          expression: '^\s*(?P<subsystem>\w+):'
          action_on_failure: pass

      - labels:
          subsystem:

  # ===================================================
  # SCRAPE CONFIG 11: Systemd Journal (Optional)
  # ===================================================
  # Alternative to file-based logs, reads from journald
  # Requires Promtail to run with systemd journal access
  - job_name: journal
    journal:
      path: /var/log/journal
      max_age: 12h
      labels:
        job: systemd-journal
        host: ${HOSTNAME}

    relabel_configs:
      # Extract systemd unit name
      - source_labels: ['__journal__systemd_unit']
        target_label: 'unit'

      # Extract syslog identifier
      - source_labels: ['__journal_syslog_identifier']
        target_label: 'syslog_identifier'

      # Extract priority (log level)
      - source_labels: ['__journal_priority_keyword']
        target_label: 'level'

  # ===================================================
  # SCRAPE CONFIG 12: Custom Application Logs
  # ===================================================
  # Generic scraper for any custom application logs
  # Assumes plain text format, one line per log entry
  - job_name: custom_apps
    static_configs:
      - targets:
          - localhost
        labels:
          job: custom_apps
          host: ${HOSTNAME}
          __path__: /var/log/custom-apps/*.log

    pipeline_stages:
      # Try to extract timestamp if present
      - regex:
          expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2})'
          action_on_failure: pass

      - timestamp:
          source: timestamp
          format: '2006-01-02T15:04:05'
          action_on_failure: fudge

      # Try to extract log level
      - regex:
          expression: '\b(?P<level>DEBUG|INFO|WARN|ERROR|FATAL)\b'
          action_on_failure: pass

      - labels:
          level:

# =====================================================
# TARGETS (Optional, for service discovery)
# =====================================================
# Can replace static_configs with dynamic targets from:
# - Kubernetes service discovery
# - Docker service discovery
# - Consul service discovery
# - File-based service discovery
#
# Example file-based SD:
# target_config:
#   sync_period: 10s
#
# scrape_configs:
#   - job_name: dynamic
#     file_sd_configs:
#       - files:
#           - /etc/promtail/targets/*.yml
#
# targets/*.yml format:
# - targets:
#     - localhost
#   labels:
#     job: app1
#     __path__: /var/log/app1/*.log

# =====================================================
# OPERATIONAL NOTES
# =====================================================
#
# Health Check:
# curl http://localhost:9080/ready
# curl http://localhost:9080/metrics
#
# Test Configuration:
# promtail -config.file=promtail-config.yml -dry-run
#
# Run Promtail:
# promtail -config.file=promtail-config.yml
#
# Monitor Promtail Metrics:
# - promtail_read_bytes_total: Total bytes read from files
# - promtail_sent_bytes_total: Total bytes sent to Loki
# - promtail_dropped_entries_total: Entries dropped (investigate!)
# - promtail_file_bytes_total: Current size of tracked files
#
# Troubleshooting:
# - Logs not appearing: Check file permissions, verify __path__ glob matches
# - High memory usage: Reduce number of tracked files, add more specific paths
# - Dropped entries: Check Loki ingestion limits, verify network connectivity
# - Duplicate logs: Check positions file, ensure Promtail not running multiple times
#
# Position File:
# - Location: /tmp/positions.yaml
# - Contains: File paths and byte offsets
# - Prevents: Re-reading logs on restart
# - Backup: Important for disaster recovery
#
# Label Cardinality:
# - Keep labels low-cardinality (< 10 unique values preferred)
# - Examples of good labels: job, host, level, service
# - Examples of bad labels: user_id, request_id, trace_id, IP address
# - High-cardinality data: Keep in log message, query with filters
#
# Pipeline Performance:
# - Regex is expensive: Keep patterns simple, use action_on_failure: pass
# - JSON parsing: Fast, use when possible
# - Timestamp parsing: Try to match exact format, use fudge as fallback
# - Label extraction: Essential for filtering, but avoid over-labeling
#
# Log Volume Management:
# - Drop noisy logs: Health checks, debug logs, static assets
# - Sample high-volume logs: Use match + sample stages
# - Filter at source: Configure apps to log less verbosely
# - Aggregate in app: Log summaries instead of individual events
#
# Security Considerations:
# - Sensitive data: Mask passwords, tokens, PII in pipeline
# - File permissions: Promtail needs read access to log files
# - Network security: Loki endpoint should be firewall-protected
# - Credentials: Never log API keys, passwords, secrets
#
# Example Masking Stage:
# - replace:
#     expression: '(password|token|secret)=\S+'
#     replace: '$1=***REDACTED***'
#
# Integration with Loki:
# - Promtail pushes logs to Loki HTTP API
# - Loki URL: http://192.168.40.30:3100/loki/api/v1/push
# - Batching: Reduces HTTP requests, improves efficiency
# - Retries: Automatic retry with exponential backoff
#
# Multi-Host Deployment:
# - Run Promtail on each VM/host
# - Use unique host label per instance
# - Centralize to single Loki instance
# - Alternative: Use syslog forwarding + single Promtail
#
# Docker Deployment:
# docker run -d \
#   --name promtail \
#   -v /var/log:/var/log:ro \
#   -v /var/lib/docker/containers:/var/lib/docker/containers:ro \
#   -v $(pwd)/promtail-config.yml:/etc/promtail/config.yml \
#   grafana/promtail:2.9.0 \
#   -config.file=/etc/promtail/config.yml
#
# Systemd Service:
# [Unit]
# Description=Promtail Log Shipper
# After=network.target
#
# [Service]
# Type=simple
# User=promtail
# ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/config.yml
# Restart=always
#
# [Install]
# WantedBy=multi-user.target
#
# =====================================================
