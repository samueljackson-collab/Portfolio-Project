# Proxmox VE Storage Configuration
# Multi-tier storage with local, distributed, and network storage
# Version: Proxmox VE 8.x
# Last Updated: 2025-11-05

# Directory: Local Storage (per-node, not shared)
# Each node has its own local storage for ISO images and VM templates
# Path: /var/lib/vz
# Use case: ISO images, container templates, temporary storage
dir: local
	path /var/lib/vz
	content vztmpl,iso,backup
	maxfiles 3
	shared 0
	# Enable pruning to keep only 3 most recent backups

# LVM: Local LVM Storage (per-node, high performance)
# Each node has dedicated LVM volume group for VM disks
# Use case: High-performance VM disks, containers
# Node 1
lvmthin: local-lvm-pve01
	vgname pve01
	thinpool data
	content rootdir,images
	nodes proxmox-01
	shared 0

# Node 2
lvmthin: local-lvm-pve02
	vgname pve02
	thinpool data
	content rootdir,images
	nodes proxmox-02
	shared 0

# Node 3
lvmthin: local-lvm-pve03
	vgname pve03
	thinpool data
	content rootdir,images
	nodes proxmox-03
	shared 0

# Ceph RBD: Distributed Storage (shared, replicated)
# 3-node Ceph cluster for high-availability storage
# Replication: 3 copies across all nodes
# Use case: HA VMs, live migration
rbd: ceph-rbd
	content images,rootdir
	krbd 0
	pool rbd
	username admin
	# Ceph monitor addresses
	monhost 192.168.40.10:6789,192.168.40.11:6789,192.168.40.12:6789
	shared 1
	nodes proxmox-01,proxmox-02,proxmox-03
	# Performance options
	# Write-back cache for better write performance
	# Note: Requires Ceph OSDs on each node

# Ceph FS: Distributed File System (shared)
# Ceph filesystem for shared file storage
# Use case: Shared directories, container templates, backups
cephfs: ceph-fs
	path /mnt/pve/ceph-fs
	content vztmpl,iso,snippets,backup
	monhost 192.168.40.10:6789,192.168.40.11:6789,192.168.40.12:6789
	shared 1
	nodes proxmox-01,proxmox-02,proxmox-03
	# Metadata server (MDS) required for CephFS

# NFS: TrueNAS Network Storage (shared)
# Primary storage backend for backups and bulk storage
# Use case: Backups, media, large files, templates
nfs: truenas-nfs
	export /mnt/tank/proxmox/backups
	path /mnt/pve/truenas-backups
	server 192.168.40.20
	content backup,vztmpl,iso
	maxfiles 10
	options vers=4.2,soft,timeo=100
	shared 1
	nodes proxmox-01,proxmox-02,proxmox-03
	# NFS v4.2 for better performance and security
	# Soft mount with timeout for graceful degradation

# NFS: TrueNAS Media Storage (shared, optional)
# Storage for media files and large datasets
# Use case: Media libraries, datasets
nfs: truenas-media
	export /mnt/tank/media
	path /mnt/pve/truenas-media
	server 192.168.40.20
	content images
	options vers=4.2,soft,timeo=100
	shared 1
	nodes proxmox-01,proxmox-02,proxmox-03
	# Read-optimized for media streaming

# iSCSI: TrueNAS Block Storage (shared, high performance)
# iSCSI target for high-performance VM storage
# Use case: Database VMs, high-IOPS workloads
# Note: Requires multipath for HA
iscsi: truenas-iscsi
	portal 192.168.40.20
	target iqn.2024-01.local.homelab:proxmox-storage
	content images
	nodes proxmox-01,proxmox-02,proxmox-03
	shared 1
	# Enable multipath in /etc/multipath.conf for redundancy

# LVM over iSCSI: Thin-provisioned iSCSI storage
# Combines iSCSI with LVM thin provisioning
# Use case: Efficient use of iSCSI storage with snapshots
lvmthin: truenas-iscsi-thin
	thinpool proxmox-thin
	vgname iscsi-vg
	content rootdir,images
	nodes proxmox-01,proxmox-02,proxmox-03
	shared 1

# Proxmox Backup Server (PBS)
# Dedicated backup storage with deduplication
# Use case: Efficient backups with incremental deduplication
pbs: proxmox-backup
	server 192.168.40.50
	datastore proxmox-backups
	username admin@pbs
	# Password stored in /etc/pve/priv/storage/<name>.pw
	content backup
	fingerprint SHA256:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX
	# Fingerprint must match PBS server certificate
	maxfiles 0
	# PBS handles its own retention policies
	prune-backups keep-all=1
	shared 1
	nodes proxmox-01,proxmox-02,proxmox-03

# ZFS over iSCSI: Alternative high-performance storage
# ZFS on TrueNAS exposed via iSCSI
# Use case: ZFS features (snapshots, compression) with block storage
# Note: Configured on TrueNAS side, accessed via iSCSI

# Storage Tier Strategy:
# Tier 1 (Fastest): Local LVM-thin on NVMe/SSD
#   - VM boot disks
#   - High-IOPS databases
#   - Cache drives
#
# Tier 2 (Balanced): Ceph RBD distributed storage
#   - HA VMs requiring live migration
#   - Application servers
#   - Clustered services
#
# Tier 3 (Capacity): NFS/iSCSI from TrueNAS
#   - Backups
#   - Media libraries
#   - Bulk storage
#   - Archival data

# Best Practices:
# 1. Use local-lvm for VMs that don't need HA
# 2. Use Ceph RBD for HA VMs that need live migration
# 3. Use NFS for backups and shared files
# 4. Use iSCSI for high-performance shared storage
# 5. Enable regular backups to PBS
# 6. Monitor storage usage and performance
# 7. Test restore procedures regularly

# Backup Schedule Recommendations:
# Critical VMs: Daily full to PBS, 7 daily + 4 weekly + 12 monthly
# Standard VMs: Weekly full to NFS, 4 weekly + 3 monthly
# Development VMs: Weekly full to local, 2 weekly retention

# Performance Optimization:
# - Use writeback cache for Ceph (requires UPS)
# - Enable discard/TRIM on SSD storage
# - Use virtio-scsi for better performance
# - Enable I/O thread for disk-intensive VMs
# - Use thick provisioning for databases
# - Monitor latency with 'iostat' and Proxmox metrics

# Maintenance:
# - Regular scrubbing of Ceph pools (weekly)
# - Monitor Ceph health: 'ceph -s'
# - Check NFS connectivity: 'showmount -e 192.168.40.20'
# - Verify iSCSI multipath: 'multipath -ll'
# - Review storage logs: /var/log/pve/tasks/
