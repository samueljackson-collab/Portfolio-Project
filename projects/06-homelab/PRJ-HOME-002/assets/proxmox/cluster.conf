# Proxmox VE Cluster Configuration
# Three-node high-availability cluster with Corosync and HA services
# Version: Proxmox VE 8.x
# Last Updated: 2025-11-05

# Cluster Information
cluster {
  # Cluster name
  name: homelab-cluster
  
  # Configuration version (auto-incremented by Corosync)
  config_version: 3
}

# Corosync Multicast Configuration
totem {
  # Corosync version
  version: 2
  
  # Cluster name for identification
  cluster_name: homelab-cluster
  
  # Network transport protocol
  # Options: udpu (unicast), udp (multicast)
  transport: udpu
  
  # Token timeout (milliseconds)
  # Time before a node is considered dead
  token: 3000
  
  # Token retransmit timeout
  token_retransmits_before_loss_const: 10
  
  # Join timeout
  join: 60
  
  # Consensus timeout
  consensus: 3600
  
  # Interface configuration
  interface {
    # Interface number (0 = primary)
    ringnumber: 0
    
    # Bind to specific network interface
    bindnetaddr: 192.168.40.0
    
    # Multicast address (not used with udpu, but required)
    mcastaddr: 239.255.1.1
    
    # Multicast port
    mcastport: 5405
    
    # Time-to-live for multicast packets
    ttl: 1
  }
}

# Logging Configuration
logging {
  # Log to system log
  to_syslog: yes
  
  # Log to file
  to_logfile: yes
  logfile: /var/log/corosync/corosync.log
  
  # Log to stderr (useful for debugging)
  to_stderr: no
  
  # Timestamp format
  timestamp: on
  
  # Debug logging (off in production)
  debug: off
  
  # Log level per subsystem
  logger_subsys {
    subsys: QUORUM
    debug: off
  }
}

# Quorum Configuration
quorum {
  # Quorum provider (corosync_votequorum for most setups)
  provider: corosync_votequorum
  
  # Expected votes (number of nodes)
  expected_votes: 3
  
  # Two-node cluster mode (disabled for 3-node cluster)
  two_node: 0
  
  # Wait for all nodes before starting
  # Set to 1 only during initial setup
  wait_for_all: 0
  
  # Last man standing (LMS) feature
  # Allows single node to maintain quorum in disaster
  last_man_standing: 0
  
  # Auto-tie breaker (ATB)
  # Uses lowest node ID as tie breaker
  auto_tie_breaker: 1
}

# Node List
# Node 1: proxmox-01
nodelist {
  node {
    # Node name (must match hostname)
    name: proxmox-01
    
    # Node ID (unique, 1-based)
    nodeid: 1
    
    # Quorum votes for this node
    quorum_votes: 1
    
    # Ring 0 address (primary network)
    ring0_addr: 192.168.40.10
  }
  
  # Node 2: proxmox-02
  node {
    name: proxmox-02
    nodeid: 2
    quorum_votes: 1
    ring0_addr: 192.168.40.11
  }
  
  # Node 3: proxmox-03
  node {
    name: proxmox-03
    nodeid: 3
    quorum_votes: 1
    ring0_addr: 192.168.40.12
  }
}

# High Availability Groups
# HA groups define resource allocation preferences

# Group: ha-group-1 (Proxmox-01 preferred)
# Priority order for VM placement
# Used for VMs that should prefer Node 1
ha_group: ha-group-1 {
  nodes: proxmox-01:100,proxmox-02:50,proxmox-03:50
  nofailback: 0
  restricted: 0
  comment: "Primary HA group with Node 1 preference"
}

# Group: ha-group-balanced (Balanced across all nodes)
# No preference, balanced distribution
ha_group: ha-group-balanced {
  nodes: proxmox-01:100,proxmox-02:100,proxmox-03:100
  nofailback: 0
  restricted: 0
  comment: "Balanced HA group"
}

# High Availability Resources
# Define which VMs/containers are HA-enabled

# Critical Infrastructure VMs
ha_resource: vm:100 {
  group: ha-group-1
  state: started
  max_restart: 3
  max_relocate: 3
  comment: "FreeIPA/RADIUS - Critical Auth Service"
}

ha_resource: vm:101 {
  group: ha-group-balanced
  state: started
  max_restart: 3
  max_relocate: 3
  comment: "Pi-hole DNS - Critical DNS Service"
}

ha_resource: vm:102 {
  group: ha-group-balanced
  state: started
  max_restart: 2
  max_relocate: 2
  comment: "Nginx Reverse Proxy"
}

ha_resource: vm:103 {
  group: ha-group-balanced
  state: started
  max_restart: 2
  max_relocate: 2
  comment: "Centralized Syslog Server"
}

# Fencing Configuration (Watchdog)
# Hardware watchdog for automatic node recovery
watchdog {
  # Watchdog device
  device: /dev/watchdog
  
  # Action on watchdog timeout
  action: reboot
}

# Cluster Network Configuration
# Separate networks for cluster traffic and migration

# Cluster network (Corosync traffic)
cluster_network: cluster {
  network: 192.168.40.0/24
  priority: 1
}

# Migration network (live migration traffic)
# Can be same as cluster network or separate
migration_network: migration {
  network: 192.168.40.0/24
  priority: 2
}

# Cluster Settings
# Additional cluster-wide settings

# Cluster resource manager settings
crm_config: cib-bootstrap-options {
  # Cluster name
  cluster-name: homelab-cluster
  
  # Enable stonith (fencing)
  stonith-enabled: false
  
  # No quorum policy
  # Options: stop, freeze, ignore, suicide
  no-quorum-policy: stop
  
  # Symmetric cluster (all nodes can run all resources)
  symmetric-cluster: true
  
  # Default resource stickiness
  # Preference to keep resources on current node
  default-resource-stickiness: 100
  
  # Migration threshold
  # Number of failures before resource is moved
  migration-threshold: 3
}

# Fencing Devices (if using IPMI or similar)
# Commented out as not all homelabs have this
# stonith:ipmi-proxmox-01 {
#   primitive: stonith:fence_ipmilan
#   params: {
#     ipaddr: 192.168.40.110
#     login: admin
#     passwd: password
#     pcmk_host_list: proxmox-01
#   }
# }

# Notes:
# 1. This configuration assumes all nodes are on the same subnet
# 2. For production, consider separate network for Corosync traffic
# 3. Firewall ports required: 5404-5412 (UDP), 22 (SSH), 8006 (Web UI)
# 4. Ensure time synchronization (NTP) across all nodes
# 5. Regular backups of cluster configuration recommended
